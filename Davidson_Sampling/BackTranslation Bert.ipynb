{"cells":[{"cell_type":"markdown","metadata":{"id":"WDlo3pPprIxl"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"n9nw6IBPthFg","outputId":"ae3f5ccb-8ca6-4772-be64-1260645c2740"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CEOVbUS2tM1-"},"outputs":[],"source":["import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RbgaWVF2dSA2"},"outputs":[],"source":["import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"neH8X0FoVt7F"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cxsqrEahtgS2"},"outputs":[],"source":["TRAIN_DATASET_PATH = \"/content/drive/MyDrive/FYP/Davidson_Sampling/train_new.csv\"\n","VALID_DATASET_PATH = \"/content/drive/MyDrive/FYP/Davidson_Sampling/valid_new.csv\"\n","TEST_DATASET_PATH = \"/content/drive/MyDrive/FYP/Davidson_Sampling/test.csv\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AXYPRZxmt_VC"},"outputs":[],"source":["train_dataset= pd.read_csv(TRAIN_DATASET_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dXmxCa8LvAdO"},"outputs":[],"source":["train_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"22eiRmmguMb_"},"outputs":[],"source":["test_dataset = pd.read_csv(TEST_DATASET_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8aaxjy-PqHXd"},"outputs":[],"source":["valid_dataset = pd.read_csv(VALID_DATASET_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"la-j52HouUbS"},"outputs":[],"source":["# Load model directly\n","from transformers import AutoTokenizer, AutoModel\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n","model = AutoModel.from_pretrained(\"bert-base-cased\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BH3ck85TsQKN"},"outputs":[],"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P0i0lQsKsR__"},"outputs":[],"source":["model.to(device)\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HrQJD0xMxyKG"},"outputs":[],"source":["seq_len = [len(tweet.split()) for tweet in train_dataset['tweet']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cE6BeUOtzMcj"},"outputs":[],"source":["pd.Series(seq_len).hist(bins=40,color='firebrick')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7HqSyqJg8Lur"},"outputs":[],"source":["MAX_LENGTH = 20"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gfHrW4-gWkA-"},"outputs":[],"source":["from torch.utils.data import DataLoader"]},{"cell_type":"markdown","metadata":{"id":"9ySBJDzVcCRz"},"source":["#Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NJRgscnhuQc9"},"outputs":[],"source":["train_tokens= tokenizer.batch_encode_plus(train_dataset[\"tweet\"].tolist(),padding='max_length',max_length=MAX_LENGTH,truncation=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6mo5hpububPO"},"outputs":[],"source":["# import torch\n","# train_seq = torch.tensor(train_tokens['input_ids'])\n","# train_mask = torch.tensor(train_tokens['attention_mask'])\n","# train_y = torch.tensor(train_dataset['label'].tolist())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"01dGFzuuWG7G"},"outputs":[],"source":["# tokenized_train_datasets=train_tokens.remove_columns(['text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r_TQPwTRWQ9E"},"outputs":[],"source":["# tokenized_train_datasets=t.with_format(\"torch\")\n","from torch.utils.data import TensorDataset\n","\n","# Assuming train_tokens contains the tokenized and encoded inputs\n","input_ids = torch.tensor(train_tokens['input_ids'])\n","attention_masks = torch.tensor(train_tokens['attention_mask'])\n","\n","# Create a TensorDataset\n","train_new_dataset = TensorDataset(input_ids, attention_masks)\n","\n","# Use the DataLoader with the TensorDataset\n","train_dataloader = DataLoader(train_new_dataset, batch_size=400, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rlpnNQgEGLRz"},"outputs":[],"source":["train_dataset['label']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eoLkCYjLNjgn"},"outputs":[],"source":["train_dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NwyTBqAG7CBx"},"outputs":[],"source":["# train_seq = torch.tensor(train_tokens['input_ids'])\n","# train_mask = torch.tensor(train_tokens['attention_mask'])\n","# train_y = torch.tensor(train_dataset['label'].tolist())\n","\n","# test_seq = torch.tensor(test_tokens['input_ids'])\n","# test_mask = torch.tensor(test_tokens['attention_mask'])\n","# test_y = torch.tensor(test_dataset['label'].tolist())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M-wynfrpYYSo"},"outputs":[],"source":["all_outputs_train_tensor = torch.tensor([], device=device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QH8ZxrpTaoGl"},"outputs":[],"source":["# for batch in train_dataloader:\n","#         print(batch)\n","#         batch = {k: v.to(device) for k, v in batch.items()}\n","#         with torch.no_grad():\n","#           outs = model(batch['input_ids'], token_type_ids=None, attention_mask=batch['attention_mask'])\n","#           bert_cls_hidden_state= outs[1]\n","#           print(bert_cls_hidden_state)\n","#           all_outputs_train_tensor = torch.cat([all_outputs_train_tensor, bert_cls_hidden_state], dim=0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g-4Cr6gpAOKr"},"outputs":[],"source":["for batch in train_dataloader:\n","    input_ids, attention_masks = batch\n","    input_ids = input_ids.to(device)\n","    attention_masks = attention_masks.to(device)\n","\n","    with torch.no_grad():\n","        outs = model(input_ids, token_type_ids=None, attention_mask=attention_masks)\n","        bert_cls_hidden_state = outs[1]\n","        all_outputs_train_tensor = torch.cat([all_outputs_train_tensor, bert_cls_hidden_state], dim=0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h3n8BYC_bORe"},"outputs":[],"source":["all_outputs_train_cpu = all_outputs_train_tensor.cpu()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M29VJRLvbQfz"},"outputs":[],"source":["all_outputs_train_numpy = all_outputs_train_cpu.numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6C5vVmtlbQ-B"},"outputs":[],"source":["all_outputs_train_numpy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g-koX3U_Fe43"},"outputs":[],"source":["import pandas as pd\n","\n","# Assuming train_dataset['label'] is a numpy array\n","labels = train_dataset['label']\n","\n","# Convert numpy arrays to pandas DataFrames\n","features_df = pd.DataFrame(all_outputs_train_numpy)\n","labels_df = pd.DataFrame(labels)\n","\n","# Concatenate features and labels along columns axis\n","combined_df = pd.concat([features_df, labels_df], axis=1)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6dtDJkBtGgmZ"},"outputs":[],"source":["combined_df"]},{"cell_type":"markdown","metadata":{"id":"b4Uad_oab5ky"},"source":["#Test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AYrrF_pybpP9"},"outputs":[],"source":["test_tokens = tokenizer.batch_encode_plus(test_dataset['tweet'].tolist(),padding='max_length',max_length=MAX_LENGTH,truncation=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8hXlCAssbpuE"},"outputs":[],"source":["# tokenized_train_datasets=t.with_format(\"torch\")\n","from torch.utils.data import TensorDataset\n","\n","# Assuming train_tokens contains the tokenized and encoded inputs\n","input_ids = torch.tensor(test_tokens['input_ids'])\n","attention_masks = torch.tensor(test_tokens['attention_mask'])\n","\n","# Create a TensorDataset\n","test_new_dataset = TensorDataset(input_ids, attention_masks)\n","\n","# Use the DataLoader with the TensorDataset\n","test_dataloader = DataLoader(test_new_dataset, batch_size=400, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l4hLGcPTbsCi"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VSGnkOxVbtwb"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L0ch7MQGbv8a"},"outputs":[],"source":["all_outputs_test_tensor = torch.tensor([], device=device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KJPLa5wTby5q"},"outputs":[],"source":["# for batch in test_dataloader:\n","#         batch = {k: v.to(device) for k, v in batch.items()}\n","#         with torch.no_grad():\n","#           outs = model(batch['input_ids'], token_type_ids=None, attention_mask=batch['attention_mask'])\n","#           b_logit_pred = outs.logits\n","\n","#           pred_label = torch.sigmoid(b_logit_pred)\n","\n","#           all_outputs_test_tensor = torch.cat([all_outputs_test_tensor, pred_label], dim=0)\n","#           print(all_outputs_test_tensor)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-VsD5hKTCIDW"},"outputs":[],"source":["for batch in test_dataloader:\n","    input_ids, attention_masks = batch\n","    input_ids = input_ids.to(device)\n","    attention_masks = attention_masks.to(device)\n","\n","    with torch.no_grad():\n","        outs = model(input_ids, token_type_ids=None, attention_mask=attention_masks)\n","        bert_cls_hidden_state = outs[1]\n","        all_outputs_test_tensor = torch.cat([all_outputs_test_tensor, bert_cls_hidden_state], dim=0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MSa0fcS6cRSG"},"outputs":[],"source":["all_outputs_test_cpu = all_outputs_test_tensor.cpu()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8eVG6o10cTgy"},"outputs":[],"source":["all_outputs_test_numpy = all_outputs_test_cpu.numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EAYIPY_DGrca"},"outputs":[],"source":["import pandas as pd\n","\n","# Assuming train_dataset['label'] is a numpy array\n","labels = test_dataset['label']\n","\n","# Convert numpy arrays to pandas DataFrames\n","features_test_df = pd.DataFrame(all_outputs_test_numpy)\n","labels_test_df = pd.DataFrame(labels)\n","\n","# Concatenate features and labels along columns axis\n","combined_test_df = pd.concat([features_test_df, labels_test_df], axis=1)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C3sASfQRHDv9"},"outputs":[],"source":["combined_test_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9rp6nfcKKrMC"},"outputs":[],"source":["# with torch.no_grad():\n","#     train_outputs = model(input_ids=train_seq, attention_mask=train_mask)\n","\n","# # Extract the embeddings from the BERT model\n","# train_embeddings = train_outputs.last_hidden_state[:, 0, :].numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ywS23UodHfIo"},"outputs":[],"source":["x_train_features = combined_df.iloc[:, :-1]\n","y_train_lables = combined_df['label']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TvS09RbJINjR"},"outputs":[],"source":["x_test_features = combined_test_df.iloc[:, :-1]\n","y_test_lables = combined_test_df['label']"]},{"cell_type":"markdown","metadata":{"id":"Uf819J63qsR3"},"source":["#Valid"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TrudhB95quDJ"},"outputs":[],"source":["valid_tokens = tokenizer.batch_encode_plus(valid_dataset['tweet'].tolist(),padding='max_length',max_length=MAX_LENGTH,truncation=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cGP_h8xJqxtD"},"outputs":[],"source":["# tokenized_train_datasets=t.with_format(\"torch\")\n","from torch.utils.data import TensorDataset\n","\n","# Assuming train_tokens contains the tokenized and encoded inputs\n","input_ids = torch.tensor(valid_tokens['input_ids'])\n","attention_masks = torch.tensor(valid_tokens['attention_mask'])\n","\n","# Create a TensorDataset\n","valid_new_dataset = TensorDataset(input_ids, attention_masks)\n","\n","# Use the DataLoader with the TensorDataset\n","valid_dataloader = DataLoader(valid_new_dataset, batch_size=400, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7mrN9U8mq1Im"},"outputs":[],"source":["all_outputs_valid_tensor = torch.tensor([], device=device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8FNEJ8X0q5aw"},"outputs":[],"source":["for batch in valid_dataloader:\n","    input_ids, attention_masks = batch\n","    input_ids = input_ids.to(device)\n","    attention_masks = attention_masks.to(device)\n","\n","    with torch.no_grad():\n","        outs = model(input_ids, token_type_ids=None, attention_mask=attention_masks)\n","        bert_cls_hidden_state = outs[1]\n","        all_outputs_valid_tensor = torch.cat([all_outputs_valid_tensor, bert_cls_hidden_state], dim=0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qq7tCDJPq8sp"},"outputs":[],"source":["all_outputs_valid_cpu = all_outputs_valid_tensor.cpu()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cEOjucy-q-Nm"},"outputs":[],"source":["all_outputs_valid_numpy = all_outputs_valid_cpu.numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"umqsfr5vq_eG"},"outputs":[],"source":["import pandas as pd\n","\n","# Assuming train_dataset['label'] is a numpy array\n","labels = valid_dataset['label']\n","\n","# Convert numpy arrays to pandas DataFrames\n","features_valid_df = pd.DataFrame(all_outputs_valid_numpy)\n","labels_valid_df = pd.DataFrame(labels)\n","\n","# Concatenate features and labels along columns axis\n","combined_valid_df = pd.concat([features_valid_df, labels_valid_df], axis=1)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W14rHkNZrBgT"},"outputs":[],"source":["combined_valid_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hrJFVP1erE7n"},"outputs":[],"source":["x_valid_features = combined_valid_df.iloc[:, :-1]\n","y_valid_lables = combined_valid_df['label']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-V1DDAFtTgrh"},"outputs":[],"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2eVOjJc_TinF"},"outputs":[],"source":["def score(y_test_lables,y_pred):\n","  accuracy = accuracy_score(y_test_lables, y_pred)\n","  precision = precision_score(y_test_lables, y_pred,average='macro')\n","  recall = recall_score(y_test_lables, y_pred,average='macro')\n","  f1 = f1_score(y_test_lables, y_pred,average='macro')\n","  conf_matrix = confusion_matrix(y_test_lables, y_pred)\n","  return accuracy,precision,recall,f1,conf_matrix"]},{"cell_type":"markdown","metadata":{"id":"By_7Q7WJJHjn"},"source":["#LogisticRegression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o1y3jPXlBqFY"},"outputs":[],"source":["# from sklearn.linear_model import LogisticRegression\n","# from sklearn.model_selection import GridSearchCV\n","# logistic_regression = LogisticRegression()\n","# param_grid = {\n","#     'C': [0.001, 0.01, 0.1],\n","#     'penalty': ['l1', 'l2'],\n","#     'solver': ['liblinear','saga']\n","# }\n","\n","# grid_search = GridSearchCV(estimator=logistic_regression, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n","\n","# grid_search.fit(x_valid_features, y_valid_lables)\n","\n","# print(\"Best Parameters:\", grid_search.best_params_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gOkZcL2uT--I"},"outputs":[],"source":["# y_pred = grid_search.predict(x_test_features)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s864GwMFTpED"},"outputs":[],"source":["# accuracy,precision,recall,f1,conf_matrix = score (y_test_lables,y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tr4XRiUZTrkF"},"outputs":[],"source":["# print(f1)\n","# print(accuracy)\n","# print(recall)\n","# print(precision)\n","# print(conf_matrix)"]},{"cell_type":"markdown","metadata":{"id":"wSY-P0hLJCuD"},"source":["#XGBoost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9YSJUKNXS1xV"},"outputs":[],"source":["# !pip install xgboost\n","# from xgboost import XGBClassifier\n","# from sklearn.model_selection import GridSearchCV\n","\n","# # Define the hyperparameter grid\n","# param_grid = {\n","#     'max_depth': [3, 5, 7],\n","#     'learning_rate': [0.1, 0.01, 0.001],\n","#     'subsample': [0.5, 0.7, 1]\n","# }\n","\n","# # Create the XGBoost model object\n","# xgb_model = XGBClassifier()\n","\n","# # Create the GridSearchCV object\n","# grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='accuracy')\n","\n","# # Fit the GridSearchCV object to the training data\n","# grid_search.fit(x_valid_features, y_valid_lables)\n","\n","# # Print the best set of hyperparameters and the corresponding score\n","# print(\"Best set of hyperparameters: \", grid_search.best_params_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AuuGlCVUTuep"},"outputs":[],"source":["# y_pred = grid_search.predict(x_test_features)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_F-RUzyBI0xH"},"outputs":[],"source":["# accuracy,precision,recall,f1,conf_matrix = score(y_test_lables,y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oEDefWYzTzBQ"},"outputs":[],"source":["# print(f1)\n","# print(accuracy)\n","# print(recall)\n","# print(precision)\n","# print(conf_matrix)"]},{"cell_type":"markdown","metadata":{"id":"4_-Iv2jNR_Y6"},"source":["#KNN"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":27,"status":"aborted","timestamp":1713800586691,"user":{"displayName":"Thushalya Weerasuriya","userId":"04336897406715178981"},"user_tz":-330},"id":"Xl-z--WGR9Qh","outputId":"73e58f58-befb-41bd-ca70-1842e4c78b35"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid = os.fork()\n","/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid = os.fork()\n"]},{"name":"stdout","output_type":"stream","text":["Best Parameters: {'metric': 'manhattan', 'n_neighbors': 9, 'weights': 'distance'}\n"]}],"source":["from sklearn.neighbors import KNeighborsClassifier\n","\n","knn = KNeighborsClassifier()\n","\n","# Define hyperparameters to tune\n","param_grid = {\n","    'n_neighbors': [3, 5, 7, 9],\n","    'weights': ['uniform', 'distance'],\n","    'metric': ['euclidean', 'manhattan']\n","}\n","\n","grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n","grid_search.fit(x_valid_features, y_valid_lables)\n","\n","print(\"Best Parameters:\", grid_search.best_params_)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"chXzpZFeU6z6"},"outputs":[],"source":["y_pred = grid_search.predict(x_test_features)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"phBmcLwGT3JR"},"outputs":[],"source":["accuracy,precision,recall,f1,conf_matrix = score (y_test_lables,y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f2TIe5dcT4zJ"},"outputs":[],"source":["print(f1)\n","print(accuracy)\n","print(recall)\n","print(precision)\n","print(conf_matrix)"]},{"cell_type":"markdown","metadata":{"id":"FsRlkJTeI_sV"},"source":["#SVC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JWp9Gd5MI3BI"},"outputs":[],"source":["#Hyper Parameter Tuning For SVC\n","from sklearn.svm import SVC\n","from sklearn.model_selection import GridSearchCV\n","\n","# defining parameter range\n","# param_grid = {'C': [0.1, 1, 10, 100, 1000],\n","#               'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n","#               'kernel': ['rbf']}\n","param_grid = {'C': [0.1, 1, 10],\n","             'gamma': [1, 0.1, 0.01],\n","             'kernel': ['rbf']}\n","\n","grid = GridSearchCV(SVC(), param_grid, cv=5, refit = True, verbose = 3)\n","\n","# fitting the model for grid search\n","grid.fit(x_valid_features, y_valid_lables)\n","grid.best_params_\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xfWpCOKuI5LI"},"outputs":[],"source":["y_pred = grid.predict(x_test_features)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QwtEYtYTJP2x"},"outputs":[],"source":["accuracy,precision,recall,f1,conf_matrix = score (y_test_lables,y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CluvsKfLrYTh"},"outputs":[],"source":["print(f1)\n","print(accuracy)\n","print(recall)\n","print(precision)\n","print(conf_matrix)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3tf8ABhPIlYn"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}