# AiLERT-FYP
Final Year Project Repository

The project, "Unmasking Hate: An Integrated Approach to Detecting Hate Speech in Social Media," aims to develop a comprehensive method for identifying hate speech on social media platforms. We believe that hate speech is a serious problem that can have harmful effects on individuals and society as a whole. By developing a system that can accurately identify instances of hate speech, we hope to promote a safer and more inclusive online environment. To achieve this goal, we plan to use a combination of natural language processing techniques and machine learning algorithms. We will collect a dataset of social media posts that have been manually labeled as either hate speech or non-hate speech. We will then preprocess the data to remove noise and irrelevant information.

The proposed methodology aims to enhance the existing Dual Contrastive Learning (DCL) framework for hate speech detection. It involves several key steps. First, the baseline DCL framework optimizes self-supervised and supervised contrastive learning losses, capturing span-level information for detecting abusive speech. This process includes using pre-trained BERT embeddings, data augmentation, and dual contrastive learning mechanisms. The DCL model is enhanced by incorporating fine-tuned BERT-based models (like BertTweet). These models improve the DCL architecture's ability to understand the context of hate speech within specific datasets. The classifier component is further enhanced through emotion modeling using DistilBERT to assign scores to eight different emotions, creating an Emotion vector. Additionally, author profiling is introduced by constructing a graph of tweet authors and applying node2vec to generate embeddings for a nuanced understanding of authors' roles in the Twitter network. The final framework combines these components, resulting in an overall enhanced framework for hate speech detection. This comprehensive approach leverages both semantic and label-based information, addressing data imbalance challenges and improving the model's performance in identifying hate speech.
