{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Owner\\anaconda3\\envs\\emotion\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"krishan-CSE/HatEval-Relabeled\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "df_train = dataset['train'].to_pandas()\n",
    "df_valid = dataset['validation'].to_pandas()\n",
    "df_test = dataset['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9088, 2)\n",
      "train value_counts:\n",
      " labels\n",
      "0    4811\n",
      "1    4277\n",
      "Name: count, dtype: int64\n",
      "                                                text  labels\n",
      "0  This human-elephant conflict has seen 13 refug...       0\n",
      "1  The awkward moment when Lexus is showing you h...       0\n",
      "2            People- why are you so fucking mean Me-       0\n",
      "3  After EU uses Turkey as buffer to stop refugee...       0\n",
      "4                           Immigration in a picture       0\n",
      "====================================================================================\n",
      "(1168, 2)\n",
      "validation value_counts:\n",
      " labels\n",
      "0    618\n",
      "1    550\n",
      "Name: count, dtype: int64\n",
      "                                                text  labels\n",
      "0  President Jokowi: it's not true millions of Ch...       0\n",
      "1  So you created the problem by mass immigration...       1\n",
      "2  I though in a free country you could worship w...       0\n",
      "3  WELP. Bitch IM JUST NOW FUCKING SEEING DUMB WHORE       1\n",
      "4  .Considering THIS , the filth on the streets o...       1\n",
      "====================================================================================\n",
      "(2724, 2)\n",
      "test value_counts:\n",
      " labels\n",
      "0    1442\n",
      "1    1282\n",
      "Name: count, dtype: int64\n",
      "                                                text  labels\n",
      "0  We have got to get these Obama DACA illegal al...       1\n",
      "1  The same bitch is all on my boos shit like gir...       0\n",
      "2  BS WILSON IS A SKANK WHORE AND A LIAR . DIDDN'...       1\n",
      "3  Immigration Expert: Trudeau Has Lost Track Of ...       1\n",
      "4  I like to delete comments that say 'first' to ...       0\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(\"train value_counts:\\n\", df_train['labels'].value_counts())\n",
    "print(df_train.head())\n",
    "print(\"==========================================\"\n",
    "      \"==========================================\")\n",
    "print(df_valid.shape)\n",
    "print(\"validation value_counts:\\n\", df_valid['labels'].value_counts())\n",
    "print(df_valid.head())\n",
    "print(\"==========================================\"\n",
    "      \"==========================================\")\n",
    "print(df_test.shape)\n",
    "print(\"test value_counts:\\n\", df_test['labels'].value_counts())\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from textblob import TextBlob\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import emoji \n",
    "def average_word_length(tweet):\n",
    "    words = tweet.split()\n",
    "    return sum(len(word) for word in words) / len(words)\n",
    "\n",
    "# def average_sentence_length(tweet):\n",
    "#     sentences = re.split(r'[.!?]+', tweet)\n",
    "#     return sum(len(word_tokenize(sentence)) for sentence in sentences) / len(sentences)\n",
    "\n",
    "def lexical_diversity(tweet):\n",
    "    words = tweet.split()\n",
    "    unique_words = set(words)\n",
    "    return len(unique_words) / len(words)\n",
    "\n",
    "def count_capital_letters(tweet):\n",
    "    return sum(1 for char in tweet if char.isupper())\n",
    "\n",
    "def count_words_surrounded_by_colons(tweet):\n",
    "    # Define a regular expression pattern to match words surrounded by ':'\n",
    "    pattern = r':(\\w+):'\n",
    "\n",
    "    # Use re.findall to find all matches in the tweet\n",
    "    matches = re.findall(pattern, tweet)\n",
    "\n",
    "    # Return the count of matched words\n",
    "    return len(matches)\n",
    "\n",
    "def count_emojis(tweet):\n",
    "    # Convert emoji symbols to their corresponding names\n",
    "    tweet_with_names = emoji.demojize(tweet)\n",
    "    return count_words_surrounded_by_colons(tweet_with_names)\n",
    "\n",
    "def hashtag_frequency(tweet):\n",
    "    hashtags = re.findall(r'#\\w+', tweet)\n",
    "    return len(hashtags)\n",
    "\n",
    "def mention_frequency(tweet):\n",
    "    mentions = re.findall(r'@\\w+', tweet)\n",
    "    return len(mentions)\n",
    "\n",
    "import string\n",
    "\n",
    "def count_special_characters(tweet):\n",
    "    special_characters = [char for char in tweet if char in string.punctuation]\n",
    "    return len(special_characters)\n",
    "\n",
    "# def capitalization_pattern(tweet):\n",
    "#     if tweet.islower():\n",
    "#         return 'All Lowercase'\n",
    "#     elif tweet.isupper():\n",
    "#         return 'All Uppercase'\n",
    "#     elif tweet.istitle():\n",
    "#         return 'Title Case'\n",
    "#     else:\n",
    "#         return 'Mixed Case'\n",
    "\n",
    "def stop_word_frequency(tweet):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in tweet.split() if word.lower() in stop_words]\n",
    "    return len(words)\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def get_linguistic_features(tweet):\n",
    "    # Tokenize the tweet\n",
    "    words = word_tokenize(tweet)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "\n",
    "    # Get parts of speech tags\n",
    "    pos_tags = pos_tag(filtered_words)\n",
    "\n",
    "    # Count various linguistic features\n",
    "    noun_count = sum(1 for word, pos in pos_tags if pos.startswith('N'))\n",
    "    verb_count = sum(1 for word, pos in pos_tags if pos.startswith('V'))\n",
    "    participle_count = sum(1 for word, pos in pos_tags if pos.startswith('V') and ('ing' in word or 'ed' in word))\n",
    "    interjection_count = sum(1 for word, pos in pos_tags if pos == 'UH')\n",
    "    pronoun_count = sum(1 for word, pos in pos_tags if pos.startswith('PRP'))\n",
    "    preposition_count = sum(1 for word, pos in pos_tags if pos.startswith('IN'))\n",
    "    adverb_count = sum(1 for word, pos in pos_tags if pos.startswith('RB'))\n",
    "    conjunction_count = sum(1 for word, pos in pos_tags if pos.startswith('CC'))\n",
    "\n",
    "    return {\n",
    "        'Noun_Count': noun_count,\n",
    "        'Verb_Count': verb_count,\n",
    "        'Participle_Count': participle_count,\n",
    "        'Interjection_Count': interjection_count,\n",
    "        'Pronoun_Count': pronoun_count,\n",
    "        'Preposition_Count': preposition_count,\n",
    "        'Adverb_Count': adverb_count,\n",
    "        'Conjunction_Count': conjunction_count\n",
    "    }\n",
    "\n",
    "import textstat\n",
    "def readability_score(tweet):\n",
    "    return textstat.flesch_reading_ease(tweet)\n",
    "\n",
    "def get_url_frequency(tweet):\n",
    "    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', tweet)\n",
    "    return len(urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This human-elephant conflict has seen 13 refug...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The awkward moment when Lexus is showing you h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>People- why are you so fucking mean Me-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>After EU uses Turkey as buffer to stop refugee...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Immigration in a picture</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9083</th>\n",
       "      <td>Ladies the moment you start hanging around wit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9084</th>\n",
       "      <td>The #AmericanCommunists who live in our countr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9085</th>\n",
       "      <td>_x0081__x0081_ Babels are also looking for a p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9086</th>\n",
       "      <td>I can lose weight, but youll always be a cunt ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9087</th>\n",
       "      <td>Please Please Please Declassify Information so...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9088 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  labels\n",
       "0     This human-elephant conflict has seen 13 refug...       0\n",
       "1     The awkward moment when Lexus is showing you h...       0\n",
       "2               People- why are you so fucking mean Me-       0\n",
       "3     After EU uses Turkey as buffer to stop refugee...       0\n",
       "4                              Immigration in a picture       0\n",
       "...                                                 ...     ...\n",
       "9083  Ladies the moment you start hanging around wit...       1\n",
       "9084  The #AmericanCommunists who live in our countr...       1\n",
       "9085  _x0081__x0081_ Babels are also looking for a p...       0\n",
       "9086  I can lose weight, but youll always be a cunt ...       1\n",
       "9087  Please Please Please Declassify Information so...       1\n",
       "\n",
       "[9088 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define a function to extract features from a single tweet\n",
    "def extract_features(tweet):\n",
    "    features = {\n",
    "        'Average_Word_Length': average_word_length(tweet),\n",
    "        # 'Average_Sentence_Length': average_sentence_length(tweet),\n",
    "        'Lexical_Diversity': lexical_diversity(tweet),\n",
    "        'Capital_Letters_Count': count_capital_letters(tweet),  # Uncomment if you want to include this feature\n",
    "        'Hashtag_Frequency': hashtag_frequency(tweet),\n",
    "        'Mention_Frequency': mention_frequency(tweet),\n",
    "        'count_emojis': count_emojis(tweet),\n",
    "        'special_chars_count': count_special_characters(tweet),\n",
    "        'Stop_Word_Frequency': stop_word_frequency(tweet),\n",
    "        **get_linguistic_features(tweet),  # Include linguistic features\n",
    "        'Readability_Score': readability_score(tweet),\n",
    "        'URL_Frequency': get_url_frequency(tweet)  # Assuming you have the correct function for this\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# Extract features for all tweets\n",
    "features_list = [extract_features(tweet) for tweet in df_train['text']]\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "features_list = pd.DataFrame(features_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Average_Word_Length</th>\n",
       "      <th>Lexical_Diversity</th>\n",
       "      <th>Capital_Letters_Count</th>\n",
       "      <th>Hashtag_Frequency</th>\n",
       "      <th>Mention_Frequency</th>\n",
       "      <th>count_emojis</th>\n",
       "      <th>special_chars_count</th>\n",
       "      <th>Stop_Word_Frequency</th>\n",
       "      <th>Noun_Count</th>\n",
       "      <th>Verb_Count</th>\n",
       "      <th>Participle_Count</th>\n",
       "      <th>Interjection_Count</th>\n",
       "      <th>Pronoun_Count</th>\n",
       "      <th>Preposition_Count</th>\n",
       "      <th>Adverb_Count</th>\n",
       "      <th>Conjunction_Count</th>\n",
       "      <th>Readability_Score</th>\n",
       "      <th>URL_Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.894737</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>78.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.437500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>88.74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.777778</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42.72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9083</th>\n",
       "      <td>4.619048</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>67.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9084</th>\n",
       "      <td>8.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9085</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>54.22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9086</th>\n",
       "      <td>3.428571</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>100.92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9087</th>\n",
       "      <td>7.631579</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9088 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Average_Word_Length  Lexical_Diversity  Capital_Letters_Count  \\\n",
       "0                6.894737           1.000000                      3   \n",
       "1                4.437500           1.000000                      2   \n",
       "2                4.000000           1.000000                      2   \n",
       "3                4.777778           0.844444                     10   \n",
       "4                5.250000           1.000000                      1   \n",
       "...                   ...                ...                    ...   \n",
       "9083             4.619048           0.857143                      4   \n",
       "9084             8.700000           1.000000                      9   \n",
       "9085             6.000000           0.970588                     10   \n",
       "9086             3.428571           0.952381                      3   \n",
       "9087             7.631579           0.894737                     40   \n",
       "\n",
       "      Hashtag_Frequency  Mention_Frequency  count_emojis  special_chars_count  \\\n",
       "0                     4                  0             0                    6   \n",
       "1                     0                  0             0                    3   \n",
       "2                     0                  0             0                    2   \n",
       "3                     0                  0             0                    6   \n",
       "4                     0                  0             0                    0   \n",
       "...                 ...                ...           ...                  ...   \n",
       "9083                  1                  0             0                    1   \n",
       "9084                  3                  0             0                    4   \n",
       "9085                  0                  0             0                   14   \n",
       "9086                  0                  0             0                    2   \n",
       "9087                  6                  0             0                    9   \n",
       "\n",
       "      Stop_Word_Frequency  Noun_Count  Verb_Count  Participle_Count  \\\n",
       "0                       3           6           2                 1   \n",
       "1                       7           4           3                 1   \n",
       "2                       4           1           1                 1   \n",
       "3                      15          14          11                 4   \n",
       "4                       2           2           0                 0   \n",
       "...                   ...         ...         ...               ...   \n",
       "9083                    9           6           2                 1   \n",
       "9084                    4           4           2                 0   \n",
       "9085                   10           8           5                 2   \n",
       "9086                   11           4           1                 0   \n",
       "9087                    4           7           5                 0   \n",
       "\n",
       "      Interjection_Count  Pronoun_Count  Preposition_Count  Adverb_Count  \\\n",
       "0                      0              0                  1             1   \n",
       "1                      0              0                  0             0   \n",
       "2                      0              0                  0             0   \n",
       "3                      0              0                  0             0   \n",
       "4                      0              0                  0             0   \n",
       "...                  ...            ...                ...           ...   \n",
       "9083                   0              0                  0             2   \n",
       "9084                   0              0                  0             0   \n",
       "9085                   0              0                  0             3   \n",
       "9086                   0              0                  0             1   \n",
       "9087                   0              0                  0             0   \n",
       "\n",
       "      Conjunction_Count  Readability_Score  URL_Frequency  \n",
       "0                     0              78.75              0  \n",
       "1                     0              80.62              0  \n",
       "2                     0              88.74              0  \n",
       "3                     0              42.72              0  \n",
       "4                     0              33.58              0  \n",
       "...                 ...                ...            ...  \n",
       "9083                  0              67.08              0  \n",
       "9084                  0              10.56              0  \n",
       "9085                  0              54.22              0  \n",
       "9086                  0             100.92              0  \n",
       "9087                  0              36.45              0  \n",
       "\n",
       "[9088 rows x 18 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>Average_Word_Length</th>\n",
       "      <th>Lexical_Diversity</th>\n",
       "      <th>Capital_Letters_Count</th>\n",
       "      <th>Hashtag_Frequency</th>\n",
       "      <th>Mention_Frequency</th>\n",
       "      <th>count_emojis</th>\n",
       "      <th>special_chars_count</th>\n",
       "      <th>Stop_Word_Frequency</th>\n",
       "      <th>Noun_Count</th>\n",
       "      <th>Verb_Count</th>\n",
       "      <th>Participle_Count</th>\n",
       "      <th>Interjection_Count</th>\n",
       "      <th>Pronoun_Count</th>\n",
       "      <th>Preposition_Count</th>\n",
       "      <th>Adverb_Count</th>\n",
       "      <th>Conjunction_Count</th>\n",
       "      <th>Readability_Score</th>\n",
       "      <th>URL_Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This human-elephant conflict has seen 13 refug...</td>\n",
       "      <td>0</td>\n",
       "      <td>6.894737</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>78.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The awkward moment when Lexus is showing you h...</td>\n",
       "      <td>0</td>\n",
       "      <td>4.437500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>People- why are you so fucking mean Me-</td>\n",
       "      <td>0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>88.74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>After EU uses Turkey as buffer to stop refugee...</td>\n",
       "      <td>0</td>\n",
       "      <td>4.777778</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42.72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Immigration in a picture</td>\n",
       "      <td>0</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9083</th>\n",
       "      <td>Ladies the moment you start hanging around wit...</td>\n",
       "      <td>1</td>\n",
       "      <td>4.619048</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>67.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9084</th>\n",
       "      <td>The #AmericanCommunists who live in our countr...</td>\n",
       "      <td>1</td>\n",
       "      <td>8.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9085</th>\n",
       "      <td>_x0081__x0081_ Babels are also looking for a p...</td>\n",
       "      <td>0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>54.22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9086</th>\n",
       "      <td>I can lose weight, but youll always be a cunt ...</td>\n",
       "      <td>1</td>\n",
       "      <td>3.428571</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>100.92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9087</th>\n",
       "      <td>Please Please Please Declassify Information so...</td>\n",
       "      <td>1</td>\n",
       "      <td>7.631579</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9088 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  labels  \\\n",
       "0     This human-elephant conflict has seen 13 refug...       0   \n",
       "1     The awkward moment when Lexus is showing you h...       0   \n",
       "2               People- why are you so fucking mean Me-       0   \n",
       "3     After EU uses Turkey as buffer to stop refugee...       0   \n",
       "4                              Immigration in a picture       0   \n",
       "...                                                 ...     ...   \n",
       "9083  Ladies the moment you start hanging around wit...       1   \n",
       "9084  The #AmericanCommunists who live in our countr...       1   \n",
       "9085  _x0081__x0081_ Babels are also looking for a p...       0   \n",
       "9086  I can lose weight, but youll always be a cunt ...       1   \n",
       "9087  Please Please Please Declassify Information so...       1   \n",
       "\n",
       "      Average_Word_Length  Lexical_Diversity  Capital_Letters_Count  \\\n",
       "0                6.894737           1.000000                      3   \n",
       "1                4.437500           1.000000                      2   \n",
       "2                4.000000           1.000000                      2   \n",
       "3                4.777778           0.844444                     10   \n",
       "4                5.250000           1.000000                      1   \n",
       "...                   ...                ...                    ...   \n",
       "9083             4.619048           0.857143                      4   \n",
       "9084             8.700000           1.000000                      9   \n",
       "9085             6.000000           0.970588                     10   \n",
       "9086             3.428571           0.952381                      3   \n",
       "9087             7.631579           0.894737                     40   \n",
       "\n",
       "      Hashtag_Frequency  Mention_Frequency  count_emojis  special_chars_count  \\\n",
       "0                     4                  0             0                    6   \n",
       "1                     0                  0             0                    3   \n",
       "2                     0                  0             0                    2   \n",
       "3                     0                  0             0                    6   \n",
       "4                     0                  0             0                    0   \n",
       "...                 ...                ...           ...                  ...   \n",
       "9083                  1                  0             0                    1   \n",
       "9084                  3                  0             0                    4   \n",
       "9085                  0                  0             0                   14   \n",
       "9086                  0                  0             0                    2   \n",
       "9087                  6                  0             0                    9   \n",
       "\n",
       "      Stop_Word_Frequency  Noun_Count  Verb_Count  Participle_Count  \\\n",
       "0                       3           6           2                 1   \n",
       "1                       7           4           3                 1   \n",
       "2                       4           1           1                 1   \n",
       "3                      15          14          11                 4   \n",
       "4                       2           2           0                 0   \n",
       "...                   ...         ...         ...               ...   \n",
       "9083                    9           6           2                 1   \n",
       "9084                    4           4           2                 0   \n",
       "9085                   10           8           5                 2   \n",
       "9086                   11           4           1                 0   \n",
       "9087                    4           7           5                 0   \n",
       "\n",
       "      Interjection_Count  Pronoun_Count  Preposition_Count  Adverb_Count  \\\n",
       "0                      0              0                  1             1   \n",
       "1                      0              0                  0             0   \n",
       "2                      0              0                  0             0   \n",
       "3                      0              0                  0             0   \n",
       "4                      0              0                  0             0   \n",
       "...                  ...            ...                ...           ...   \n",
       "9083                   0              0                  0             2   \n",
       "9084                   0              0                  0             0   \n",
       "9085                   0              0                  0             3   \n",
       "9086                   0              0                  0             1   \n",
       "9087                   0              0                  0             0   \n",
       "\n",
       "      Conjunction_Count  Readability_Score  URL_Frequency  \n",
       "0                     0              78.75              0  \n",
       "1                     0              80.62              0  \n",
       "2                     0              88.74              0  \n",
       "3                     0              42.72              0  \n",
       "4                     0              33.58              0  \n",
       "...                 ...                ...            ...  \n",
       "9083                  0              67.08              0  \n",
       "9084                  0              10.56              0  \n",
       "9085                  0              54.22              0  \n",
       "9086                  0             100.92              0  \n",
       "9087                  0              36.45              0  \n",
       "\n",
       "[9088 rows x 20 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge the features with the original dataframe\n",
    "df_train = pd.concat([df_train, features_list], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe\n",
    "df_train.to_csv('train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>Average_Word_Length</th>\n",
       "      <th>Lexical_Diversity</th>\n",
       "      <th>Capital_Letters_Count</th>\n",
       "      <th>Hashtag_Frequency</th>\n",
       "      <th>Mention_Frequency</th>\n",
       "      <th>count_emojis</th>\n",
       "      <th>special_chars_count</th>\n",
       "      <th>Stop_Word_Frequency</th>\n",
       "      <th>Noun_Count</th>\n",
       "      <th>Verb_Count</th>\n",
       "      <th>Participle_Count</th>\n",
       "      <th>Interjection_Count</th>\n",
       "      <th>Pronoun_Count</th>\n",
       "      <th>Preposition_Count</th>\n",
       "      <th>Adverb_Count</th>\n",
       "      <th>Conjunction_Count</th>\n",
       "      <th>Readability_Score</th>\n",
       "      <th>URL_Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>President Jokowi: it's not true millions of Ch...</td>\n",
       "      <td>0</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So you created the problem by mass immigration...</td>\n",
       "      <td>1</td>\n",
       "      <td>5.866667</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>46.94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I though in a free country you could worship w...</td>\n",
       "      <td>0</td>\n",
       "      <td>4.043478</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89.04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WELP. Bitch IM JUST NOW FUCKING SEEING DUMB WHORE</td>\n",
       "      <td>1</td>\n",
       "      <td>4.555556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>96.18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.Considering THIS , the filth on the streets o...</td>\n",
       "      <td>1</td>\n",
       "      <td>5.838710</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52.87</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>Basically you don't care about rape or victims...</td>\n",
       "      <td>0</td>\n",
       "      <td>4.560000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>75.71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>How is #AndrewGillum plan on paying for the Mi...</td>\n",
       "      <td>1</td>\n",
       "      <td>5.131579</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67.04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>Really do tell! Hysterical? Strange tweet you ...</td>\n",
       "      <td>0</td>\n",
       "      <td>5.357143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>58.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>In NY? Check out Immigrant Arts Coalition Summit</td>\n",
       "      <td>0</td>\n",
       "      <td>5.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>I hate bitches that are always mad at the worl...</td>\n",
       "      <td>1</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>85.02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1168 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  labels  \\\n",
       "0     President Jokowi: it's not true millions of Ch...       0   \n",
       "1     So you created the problem by mass immigration...       1   \n",
       "2     I though in a free country you could worship w...       0   \n",
       "3     WELP. Bitch IM JUST NOW FUCKING SEEING DUMB WHORE       1   \n",
       "4     .Considering THIS , the filth on the streets o...       1   \n",
       "...                                                 ...     ...   \n",
       "1163  Basically you don't care about rape or victims...       0   \n",
       "1164  How is #AndrewGillum plan on paying for the Mi...       1   \n",
       "1165  Really do tell! Hysterical? Strange tweet you ...       0   \n",
       "1166   In NY? Check out Immigrant Arts Coalition Summit       0   \n",
       "1167  I hate bitches that are always mad at the worl...       1   \n",
       "\n",
       "      Average_Word_Length  Lexical_Diversity  Capital_Letters_Count  \\\n",
       "0                5.333333           0.717949                      9   \n",
       "1                5.866667           0.966667                      4   \n",
       "2                4.043478           0.869565                      3   \n",
       "3                4.555556           1.000000                     36   \n",
       "4                5.838710           0.935484                     15   \n",
       "...                   ...                ...                    ...   \n",
       "1163             4.560000           0.960000                      5   \n",
       "1164             5.131579           0.894737                     15   \n",
       "1165             5.357143           1.000000                      7   \n",
       "1166             5.125000           1.000000                      8   \n",
       "1167             3.750000           1.000000                      1   \n",
       "\n",
       "      Hashtag_Frequency  Mention_Frequency  count_emojis  special_chars_count  \\\n",
       "0                     0                  0             0                   12   \n",
       "1                     0                  0             0                   13   \n",
       "2                     0                  0             0                    3   \n",
       "3                     0                  0             0                    1   \n",
       "4                     2                  0             0                   13   \n",
       "...                 ...                ...           ...                  ...   \n",
       "1163                  0                  0             0                    3   \n",
       "1164                  6                  0             0                   10   \n",
       "1165                  1                  0             0                    4   \n",
       "1166                  0                  0             0                    1   \n",
       "1167                  0                  0             0                    0   \n",
       "\n",
       "      Stop_Word_Frequency  Noun_Count  Verb_Count  Participle_Count  \\\n",
       "0                      13          10           3                 1   \n",
       "1                      13          10           2                 1   \n",
       "2                      10           5           2                 0   \n",
       "3                       2           4           2                 2   \n",
       "4                      13           6           3                 1   \n",
       "...                   ...         ...         ...               ...   \n",
       "1163                   12           7           3                 0   \n",
       "1164                   20           9           6                 3   \n",
       "1165                    3           2           3                 1   \n",
       "1166                    2           4           0                 0   \n",
       "1167                    9           7           1                 0   \n",
       "\n",
       "      Interjection_Count  Pronoun_Count  Preposition_Count  Adverb_Count  \\\n",
       "0                      0              0                  0             0   \n",
       "1                      0              0                  0             1   \n",
       "2                      0              0                  1             0   \n",
       "3                      0              0                  0             0   \n",
       "4                      0              0                  0             0   \n",
       "...                  ...            ...                ...           ...   \n",
       "1163                   0              0                  0             1   \n",
       "1164                   0              0                  1             0   \n",
       "1165                   0              0                  0             1   \n",
       "1166                   0              0                  0             1   \n",
       "1167                   0              0                  1             1   \n",
       "\n",
       "      Conjunction_Count  Readability_Score  URL_Frequency  \n",
       "0                     0              69.99              0  \n",
       "1                     0              46.94              0  \n",
       "2                     0              89.04              0  \n",
       "3                     0              96.18              0  \n",
       "4                     0              52.87              0  \n",
       "...                 ...                ...            ...  \n",
       "1163                  0              75.71              0  \n",
       "1164                  0              67.04              0  \n",
       "1165                  0              58.24              0  \n",
       "1166                  0              71.82              0  \n",
       "1167                  0              85.02              0  \n",
       "\n",
       "[1168 rows x 20 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do the same for the validation set\n",
    "features_list = [extract_features(tweet) for tweet in df_valid['text']]\n",
    "features_list = pd.DataFrame(features_list)\n",
    "df_valid = pd.concat([df_valid, features_list], axis=1)\n",
    "df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.to_csv('valid.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>Average_Word_Length</th>\n",
       "      <th>Lexical_Diversity</th>\n",
       "      <th>Capital_Letters_Count</th>\n",
       "      <th>Hashtag_Frequency</th>\n",
       "      <th>Mention_Frequency</th>\n",
       "      <th>count_emojis</th>\n",
       "      <th>special_chars_count</th>\n",
       "      <th>Stop_Word_Frequency</th>\n",
       "      <th>Noun_Count</th>\n",
       "      <th>Verb_Count</th>\n",
       "      <th>Participle_Count</th>\n",
       "      <th>Interjection_Count</th>\n",
       "      <th>Pronoun_Count</th>\n",
       "      <th>Preposition_Count</th>\n",
       "      <th>Adverb_Count</th>\n",
       "      <th>Conjunction_Count</th>\n",
       "      <th>Readability_Score</th>\n",
       "      <th>URL_Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We have got to get these Obama DACA illegal al...</td>\n",
       "      <td>1</td>\n",
       "      <td>5.794118</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54.22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The same bitch is all on my boos shit like gir...</td>\n",
       "      <td>0</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110.06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BS WILSON IS A SKANK WHORE AND A LIAR . DIDDN'...</td>\n",
       "      <td>1</td>\n",
       "      <td>3.625000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>89.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Immigration Expert: Trudeau Has Lost Track Of ...</td>\n",
       "      <td>1</td>\n",
       "      <td>5.033333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49.49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I like to delete comments that say 'first' to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  labels  \\\n",
       "0  We have got to get these Obama DACA illegal al...       1   \n",
       "1  The same bitch is all on my boos shit like gir...       0   \n",
       "2  BS WILSON IS A SKANK WHORE AND A LIAR . DIDDN'...       1   \n",
       "3  Immigration Expert: Trudeau Has Lost Track Of ...       1   \n",
       "4  I like to delete comments that say 'first' to ...       0   \n",
       "\n",
       "   Average_Word_Length  Lexical_Diversity  Capital_Letters_Count  \\\n",
       "0             5.794118           0.970588                     19   \n",
       "1             3.333333           1.000000                      1   \n",
       "2             3.625000           0.937500                     56   \n",
       "3             5.033333           0.933333                     20   \n",
       "4             4.500000           0.916667                      1   \n",
       "\n",
       "   Hashtag_Frequency  Mention_Frequency  count_emojis  special_chars_count  \\\n",
       "0                  4                  0             0                    8   \n",
       "1                  0                  0             0                    0   \n",
       "2                  0                  0             0                    2   \n",
       "3                  0                  0             0                    4   \n",
       "4                  0                  0             0                    2   \n",
       "\n",
       "   Stop_Word_Frequency  Noun_Count  Verb_Count  Participle_Count  \\\n",
       "0                   16           9           5                 0   \n",
       "1                    6           5           0                 0   \n",
       "2                    6           4           3                 0   \n",
       "3                   10          10           3                 1   \n",
       "4                    5           2           1                 0   \n",
       "\n",
       "   Interjection_Count  Pronoun_Count  Preposition_Count  Adverb_Count  \\\n",
       "0                   0              0                  0             0   \n",
       "1                   0              0                  1             0   \n",
       "2                   0              0                  0             1   \n",
       "3                   0              0                  0             0   \n",
       "4                   0              0                  1             0   \n",
       "\n",
       "   Conjunction_Count  Readability_Score  URL_Frequency  \n",
       "0                  0              54.22              0  \n",
       "1                  0             110.06              0  \n",
       "2                  0              89.24              0  \n",
       "3                  0              49.49              0  \n",
       "4                  0              84.68              0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do the same for the test set\n",
    "features_list = [extract_features(tweet) for tweet in df_test['text']]\n",
    "features_list = pd.DataFrame(features_list)\n",
    "df_test = pd.concat([df_test, features_list], axis=1)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
