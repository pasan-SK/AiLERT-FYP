{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"krishan-CSE/Davidson_Hate_Speech_New\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "df_train = dataset['train'].to_pandas()\n",
    "# df_valid = dataset['validation'].to_pandas()\n",
    "df_test = dataset['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18587, 2)\n",
      "train value_counts:\n",
      " labels\n",
      "0    17518\n",
      "1     1069\n",
      "Name: count, dtype: int64\n",
      "                                                text  labels\n",
      "0  @user oh I forgot you retarded lol lemme break...       0\n",
      "1  RT @user: Otw to DC. Hopefully I'll see my bes...       0\n",
      "2  Bitches dont wann fxck me no more bitches wann...       0\n",
      "3  RT @user: The paranoid fear of sharia on the r...       0\n",
      "4  Damn hoe.. You so ugly even hello kitty said g...       1\n",
      "====================================================================================\n",
      "(6196, 2)\n",
      "test value_counts:\n",
      " labels\n",
      "0    5835\n",
      "1     361\n",
      "Name: count, dtype: int64\n",
      "                                                text  labels\n",
      "0               @user oh, you cute, you fancy bitch!       0\n",
      "1  I don't buy pussy cuz it's free bitch I buy lacs!       0\n",
      "2  Such a trash episode of the walking dead tonight.       0\n",
      "3  big bitches be sayin why yo eyes all on my sto...       0\n",
      "4  RT @user: Vice President Biden &amp; his wife ...       0\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(\"train value_counts:\\n\", df_train['labels'].value_counts())\n",
    "print(df_train.head())\n",
    "print(\"==========================================\"\n",
    "      \"==========================================\")\n",
    "# print(df_valid.shape)\n",
    "# print(\"validation value_counts:\\n\", df_valid['labels'].value_counts())\n",
    "# print(df_valid.head())\n",
    "# print(\"==========================================\"\n",
    "#       \"==========================================\")\n",
    "print(df_test.shape)\n",
    "print(\"test value_counts:\\n\", df_test['labels'].value_counts())\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from textblob import TextBlob\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import emoji \n",
    "def average_word_length(tweet):\n",
    "    words = tweet.split()\n",
    "    return sum(len(word) for word in words) / len(words)\n",
    "\n",
    "# def average_sentence_length(tweet):\n",
    "#     sentences = re.split(r'[.!?]+', tweet)\n",
    "#     return sum(len(word_tokenize(sentence)) for sentence in sentences) / len(sentences)\n",
    "\n",
    "def lexical_diversity(tweet):\n",
    "    words = tweet.split()\n",
    "    unique_words = set(words)\n",
    "    return len(unique_words) / len(words)\n",
    "\n",
    "def count_capital_letters(tweet):\n",
    "    return sum(1 for char in tweet if char.isupper())\n",
    "\n",
    "def count_words_surrounded_by_colons(tweet):\n",
    "    # Define a regular expression pattern to match words surrounded by ':'\n",
    "    pattern = r':(\\w+):'\n",
    "\n",
    "    # Use re.findall to find all matches in the tweet\n",
    "    matches = re.findall(pattern, tweet)\n",
    "\n",
    "    # Return the count of matched words\n",
    "    return len(matches)\n",
    "\n",
    "def count_emojis(tweet):\n",
    "    # Convert emoji symbols to their corresponding names\n",
    "    tweet_with_names = emoji.demojize(tweet)\n",
    "    return count_words_surrounded_by_colons(tweet_with_names)\n",
    "\n",
    "def hashtag_frequency(tweet):\n",
    "    hashtags = re.findall(r'#\\w+', tweet)\n",
    "    return len(hashtags)\n",
    "\n",
    "def mention_frequency(tweet):\n",
    "    mentions = re.findall(r'@\\w+', tweet)\n",
    "    return len(mentions)\n",
    "\n",
    "import string\n",
    "\n",
    "def count_special_characters(tweet):\n",
    "    special_characters = [char for char in tweet if char in string.punctuation]\n",
    "    return len(special_characters)\n",
    "\n",
    "# def capitalization_pattern(tweet):\n",
    "#     if tweet.islower():\n",
    "#         return 'All Lowercase'\n",
    "#     elif tweet.isupper():\n",
    "#         return 'All Uppercase'\n",
    "#     elif tweet.istitle():\n",
    "#         return 'Title Case'\n",
    "#     else:\n",
    "#         return 'Mixed Case'\n",
    "\n",
    "def stop_word_frequency(tweet):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in tweet.split() if word.lower() in stop_words]\n",
    "    return len(words)\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def get_linguistic_features(tweet):\n",
    "    # Tokenize the tweet\n",
    "    words = word_tokenize(tweet)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "\n",
    "    # Get parts of speech tags\n",
    "    pos_tags = pos_tag(filtered_words)\n",
    "\n",
    "    # Count various linguistic features\n",
    "    noun_count = sum(1 for word, pos in pos_tags if pos.startswith('N'))\n",
    "    verb_count = sum(1 for word, pos in pos_tags if pos.startswith('V'))\n",
    "    participle_count = sum(1 for word, pos in pos_tags if pos.startswith('V') and ('ing' in word or 'ed' in word))\n",
    "    interjection_count = sum(1 for word, pos in pos_tags if pos == 'UH')\n",
    "    pronoun_count = sum(1 for word, pos in pos_tags if pos.startswith('PRP'))\n",
    "    preposition_count = sum(1 for word, pos in pos_tags if pos.startswith('IN'))\n",
    "    adverb_count = sum(1 for word, pos in pos_tags if pos.startswith('RB'))\n",
    "    conjunction_count = sum(1 for word, pos in pos_tags if pos.startswith('CC'))\n",
    "\n",
    "    return {\n",
    "        'Noun_Count': noun_count,\n",
    "        'Verb_Count': verb_count,\n",
    "        'Participle_Count': participle_count,\n",
    "        'Interjection_Count': interjection_count,\n",
    "        'Pronoun_Count': pronoun_count,\n",
    "        'Preposition_Count': preposition_count,\n",
    "        'Adverb_Count': adverb_count,\n",
    "        'Conjunction_Count': conjunction_count\n",
    "    }\n",
    "\n",
    "import textstat\n",
    "def readability_score(tweet):\n",
    "    return textstat.flesch_reading_ease(tweet)\n",
    "\n",
    "def get_url_frequency(tweet):\n",
    "    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', tweet)\n",
    "    return len(urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user oh I forgot you retarded lol lemme break...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @user: Otw to DC. Hopefully I'll see my bes...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bitches dont wann fxck me no more bitches wann...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @user: The paranoid fear of sharia on the r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Damn hoe.. You so ugly even hello kitty said g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18582</th>\n",
       "      <td>Would you be interested in some eggplant? RT @...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18583</th>\n",
       "      <td>Mad respect to Affleck for refusing to wear a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18584</th>\n",
       "      <td>RT @user: So ike just had to punch a nigga for...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18585</th>\n",
       "      <td>Talking shit like a hoe about huh so like a ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18586</th>\n",
       "      <td>@user same here. Fucking hated that bitch.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18587 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  labels\n",
       "0      @user oh I forgot you retarded lol lemme break...       0\n",
       "1      RT @user: Otw to DC. Hopefully I'll see my bes...       0\n",
       "2      Bitches dont wann fxck me no more bitches wann...       0\n",
       "3      RT @user: The paranoid fear of sharia on the r...       0\n",
       "4      Damn hoe.. You so ugly even hello kitty said g...       1\n",
       "...                                                  ...     ...\n",
       "18582  Would you be interested in some eggplant? RT @...       0\n",
       "18583  Mad respect to Affleck for refusing to wear a ...       0\n",
       "18584  RT @user: So ike just had to punch a nigga for...       0\n",
       "18585  Talking shit like a hoe about huh so like a ma...       0\n",
       "18586         @user same here. Fucking hated that bitch.       0\n",
       "\n",
       "[18587 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define a function to extract features from a single tweet\n",
    "def extract_features(tweet):\n",
    "    features = {\n",
    "        'Average_Word_Length': average_word_length(tweet),\n",
    "        # 'Average_Sentence_Length': average_sentence_length(tweet),\n",
    "        'Lexical_Diversity': lexical_diversity(tweet),\n",
    "        'Capital_Letters_Count': count_capital_letters(tweet),  # Uncomment if you want to include this feature\n",
    "        'Hashtag_Frequency': hashtag_frequency(tweet),\n",
    "        'Mention_Frequency': mention_frequency(tweet),\n",
    "        'count_emojis': count_emojis(tweet),\n",
    "        'special_chars_count': count_special_characters(tweet),\n",
    "        'Stop_Word_Frequency': stop_word_frequency(tweet),\n",
    "        **get_linguistic_features(tweet),  # Include linguistic features\n",
    "        'Readability_Score': readability_score(tweet),\n",
    "        'URL_Frequency': get_url_frequency(tweet)  # Assuming you have the correct function for this\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# Extract features for all tweets\n",
    "features_list = [extract_features(tweet) for tweet in df_train['text']]\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "features_list = pd.DataFrame(features_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Average_Word_Length</th>\n",
       "      <th>Lexical_Diversity</th>\n",
       "      <th>Capital_Letters_Count</th>\n",
       "      <th>Hashtag_Frequency</th>\n",
       "      <th>Mention_Frequency</th>\n",
       "      <th>count_emojis</th>\n",
       "      <th>special_chars_count</th>\n",
       "      <th>Stop_Word_Frequency</th>\n",
       "      <th>Noun_Count</th>\n",
       "      <th>Verb_Count</th>\n",
       "      <th>Participle_Count</th>\n",
       "      <th>Interjection_Count</th>\n",
       "      <th>Pronoun_Count</th>\n",
       "      <th>Preposition_Count</th>\n",
       "      <th>Adverb_Count</th>\n",
       "      <th>Conjunction_Count</th>\n",
       "      <th>Readability_Score</th>\n",
       "      <th>URL_Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.684211</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>99.53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.230769</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66.74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>57.61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>78.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18582</th>\n",
       "      <td>4.384615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>81.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18583</th>\n",
       "      <td>4.250000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>55.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18584</th>\n",
       "      <td>3.600000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71.48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18585</th>\n",
       "      <td>3.166667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>95.51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18586</th>\n",
       "      <td>5.142857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84.84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18587 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Average_Word_Length  Lexical_Diversity  Capital_Letters_Count  \\\n",
       "0                 3.684211           0.947368                      3   \n",
       "1                 4.000000           0.941176                      8   \n",
       "2                 4.230769           0.846154                      1   \n",
       "3                 5.000000           0.956522                      4   \n",
       "4                 4.400000           1.000000                      2   \n",
       "...                    ...                ...                    ...   \n",
       "18582             4.384615           1.000000                      3   \n",
       "18583             4.250000           0.875000                      5   \n",
       "18584             3.600000           0.960000                      6   \n",
       "18585             3.166667           0.833333                      2   \n",
       "18586             5.142857           1.000000                      1   \n",
       "\n",
       "       Hashtag_Frequency  Mention_Frequency  count_emojis  \\\n",
       "0                      0                  1             0   \n",
       "1                      0                  2             0   \n",
       "2                      0                  0             0   \n",
       "3                      0                  1             0   \n",
       "4                      0                  0             0   \n",
       "...                  ...                ...           ...   \n",
       "18582                  0                  1             0   \n",
       "18583                  0                  0             0   \n",
       "18584                  1                  1             0   \n",
       "18585                  0                  0             0   \n",
       "18586                  0                  1             0   \n",
       "\n",
       "       special_chars_count  Stop_Word_Frequency  Noun_Count  Verb_Count  \\\n",
       "0                        1                    9           6           2   \n",
       "1                        7                    4           7           2   \n",
       "2                        0                    5           4           3   \n",
       "3                        6                    9           7           2   \n",
       "4                        3                    2           4           1   \n",
       "...                    ...                  ...         ...         ...   \n",
       "18582                    5                    6           4           0   \n",
       "18583                    2                    8           6           4   \n",
       "18584                    3                   10           8           4   \n",
       "18585                    0                    7           6           2   \n",
       "18586                    3                    2           3           1   \n",
       "\n",
       "       Participle_Count  Interjection_Count  Pronoun_Count  Preposition_Count  \\\n",
       "0                     0                   0              0                  0   \n",
       "1                     0                   0              0                  0   \n",
       "2                     0                   0              0                  0   \n",
       "3                     0                   0              0                  0   \n",
       "4                     0                   0              0                  0   \n",
       "...                 ...                 ...            ...                ...   \n",
       "18582                 0                   0              0                  0   \n",
       "18583                 3                   0              0                  1   \n",
       "18584                 1                   0              0                  1   \n",
       "18585                 1                   0              0                  2   \n",
       "18586                 1                   0              0                  0   \n",
       "\n",
       "       Adverb_Count  Conjunction_Count  Readability_Score  URL_Frequency  \n",
       "0                 0                  0              86.03              0  \n",
       "1                 1                  0              99.53              0  \n",
       "2                 0                  0              66.74              0  \n",
       "3                 1                  0              57.61              0  \n",
       "4                 2                  0              78.25              0  \n",
       "...             ...                ...                ...            ...  \n",
       "18582             0                  0              81.80              0  \n",
       "18583             1                  0              55.58              0  \n",
       "18584             0                  0              71.48              0  \n",
       "18585             0                  0              95.51              0  \n",
       "18586             0                  0              84.84              0  \n",
       "\n",
       "[18587 rows x 18 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(features_list)\n",
    "scaled_features_df = pd.DataFrame(scaled_features, columns=features_list.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Average_Word_Length</th>\n",
       "      <th>Lexical_Diversity</th>\n",
       "      <th>Capital_Letters_Count</th>\n",
       "      <th>Hashtag_Frequency</th>\n",
       "      <th>Mention_Frequency</th>\n",
       "      <th>count_emojis</th>\n",
       "      <th>special_chars_count</th>\n",
       "      <th>Stop_Word_Frequency</th>\n",
       "      <th>Noun_Count</th>\n",
       "      <th>Verb_Count</th>\n",
       "      <th>Participle_Count</th>\n",
       "      <th>Interjection_Count</th>\n",
       "      <th>Pronoun_Count</th>\n",
       "      <th>Preposition_Count</th>\n",
       "      <th>Adverb_Count</th>\n",
       "      <th>Conjunction_Count</th>\n",
       "      <th>Readability_Score</th>\n",
       "      <th>URL_Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.141527</td>\n",
       "      <td>0.931579</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.861353</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.163539</td>\n",
       "      <td>0.923529</td>\n",
       "      <td>0.126984</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.914542</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.179625</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.785351</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.233244</td>\n",
       "      <td>0.943478</td>\n",
       "      <td>0.063492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.749379</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.191421</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.830700</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18582</th>\n",
       "      <td>0.190349</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.844687</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18583</th>\n",
       "      <td>0.180965</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>0.079365</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.741381</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18584</th>\n",
       "      <td>0.135657</td>\n",
       "      <td>0.948000</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.804027</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18585</th>\n",
       "      <td>0.105451</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.898704</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18586</th>\n",
       "      <td>0.243202</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.856664</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18587 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Average_Word_Length  Lexical_Diversity  Capital_Letters_Count  \\\n",
       "0                 0.141527           0.931579               0.047619   \n",
       "1                 0.163539           0.923529               0.126984   \n",
       "2                 0.179625           0.800000               0.015873   \n",
       "3                 0.233244           0.943478               0.063492   \n",
       "4                 0.191421           1.000000               0.031746   \n",
       "...                    ...                ...                    ...   \n",
       "18582             0.190349           1.000000               0.047619   \n",
       "18583             0.180965           0.837500               0.079365   \n",
       "18584             0.135657           0.948000               0.095238   \n",
       "18585             0.105451           0.783333               0.031746   \n",
       "18586             0.243202           1.000000               0.015873   \n",
       "\n",
       "       Hashtag_Frequency  Mention_Frequency  count_emojis  \\\n",
       "0               0.000000                0.1           0.0   \n",
       "1               0.000000                0.2           0.0   \n",
       "2               0.000000                0.0           0.0   \n",
       "3               0.000000                0.1           0.0   \n",
       "4               0.000000                0.0           0.0   \n",
       "...                  ...                ...           ...   \n",
       "18582           0.000000                0.1           0.0   \n",
       "18583           0.000000                0.0           0.0   \n",
       "18584           0.090909                0.1           0.0   \n",
       "18585           0.000000                0.0           0.0   \n",
       "18586           0.000000                0.1           0.0   \n",
       "\n",
       "       special_chars_count  Stop_Word_Frequency  Noun_Count  Verb_Count  \\\n",
       "0                 0.022222                 0.45    0.272727       0.250   \n",
       "1                 0.155556                 0.20    0.318182       0.250   \n",
       "2                 0.000000                 0.25    0.181818       0.375   \n",
       "3                 0.133333                 0.45    0.318182       0.250   \n",
       "4                 0.066667                 0.10    0.181818       0.125   \n",
       "...                    ...                  ...         ...         ...   \n",
       "18582             0.111111                 0.30    0.181818       0.000   \n",
       "18583             0.044444                 0.40    0.272727       0.500   \n",
       "18584             0.066667                 0.50    0.363636       0.500   \n",
       "18585             0.000000                 0.35    0.272727       0.250   \n",
       "18586             0.066667                 0.10    0.136364       0.125   \n",
       "\n",
       "       Participle_Count  Interjection_Count  Pronoun_Count  Preposition_Count  \\\n",
       "0                   0.0                 0.0            0.0               0.00   \n",
       "1                   0.0                 0.0            0.0               0.00   \n",
       "2                   0.0                 0.0            0.0               0.00   \n",
       "3                   0.0                 0.0            0.0               0.00   \n",
       "4                   0.0                 0.0            0.0               0.00   \n",
       "...                 ...                 ...            ...                ...   \n",
       "18582               0.0                 0.0            0.0               0.00   \n",
       "18583               0.6                 0.0            0.0               0.25   \n",
       "18584               0.2                 0.0            0.0               0.25   \n",
       "18585               0.2                 0.0            0.0               0.50   \n",
       "18586               0.2                 0.0            0.0               0.00   \n",
       "\n",
       "       Adverb_Count  Conjunction_Count  Readability_Score  URL_Frequency  \n",
       "0          0.000000                0.0           0.861353            0.0  \n",
       "1          0.166667                0.0           0.914542            0.0  \n",
       "2          0.000000                0.0           0.785351            0.0  \n",
       "3          0.166667                0.0           0.749379            0.0  \n",
       "4          0.333333                0.0           0.830700            0.0  \n",
       "...             ...                ...                ...            ...  \n",
       "18582      0.000000                0.0           0.844687            0.0  \n",
       "18583      0.166667                0.0           0.741381            0.0  \n",
       "18584      0.000000                0.0           0.804027            0.0  \n",
       "18585      0.000000                0.0           0.898704            0.0  \n",
       "18586      0.000000                0.0           0.856664            0.0  \n",
       "\n",
       "[18587 rows x 18 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the features with the original dataframe\n",
    "df_train = pd.concat([df_train, scaled_features_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe\n",
    "df_train.to_csv('train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>Average_Word_Length</th>\n",
       "      <th>Lexical_Diversity</th>\n",
       "      <th>Capital_Letters_Count</th>\n",
       "      <th>Hashtag_Frequency</th>\n",
       "      <th>Mention_Frequency</th>\n",
       "      <th>count_emojis</th>\n",
       "      <th>special_chars_count</th>\n",
       "      <th>Stop_Word_Frequency</th>\n",
       "      <th>Noun_Count</th>\n",
       "      <th>Verb_Count</th>\n",
       "      <th>Participle_Count</th>\n",
       "      <th>Interjection_Count</th>\n",
       "      <th>Pronoun_Count</th>\n",
       "      <th>Preposition_Count</th>\n",
       "      <th>Adverb_Count</th>\n",
       "      <th>Conjunction_Count</th>\n",
       "      <th>Readability_Score</th>\n",
       "      <th>URL_Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>President Jokowi: it's not true millions of Ch...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.076412</td>\n",
       "      <td>0.684766</td>\n",
       "      <td>0.039474</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.960962</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So you created the problem by mass immigration...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.087043</td>\n",
       "      <td>0.962745</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232143</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.943398</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I though in a free country you could worship w...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.050701</td>\n",
       "      <td>0.854220</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.975479</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WELP. Bitch IM JUST NOW FUCKING SEEING DUMB WHORE</td>\n",
       "      <td>1</td>\n",
       "      <td>0.060908</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.980919</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.Considering THIS , the filth on the streets o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.086486</td>\n",
       "      <td>0.927894</td>\n",
       "      <td>0.065789</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232143</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.947917</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>Basically you don't care about rape or victims...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.060997</td>\n",
       "      <td>0.955294</td>\n",
       "      <td>0.021930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.965321</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>How is #AndrewGillum plan on paying for the Mi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.072390</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.065789</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.958714</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>Really do tell! Hysterical? Strange tweet you ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.076887</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.030702</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.952009</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>In NY? Check out Immigrant Arts Coalition Summit</td>\n",
       "      <td>0</td>\n",
       "      <td>0.072259</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.035088</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.962357</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>I hate bitches that are always mad at the worl...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.044850</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.972415</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1168 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  labels  \\\n",
       "0     President Jokowi: it's not true millions of Ch...       0   \n",
       "1     So you created the problem by mass immigration...       1   \n",
       "2     I though in a free country you could worship w...       0   \n",
       "3     WELP. Bitch IM JUST NOW FUCKING SEEING DUMB WHORE       1   \n",
       "4     .Considering THIS , the filth on the streets o...       1   \n",
       "...                                                 ...     ...   \n",
       "1163  Basically you don't care about rape or victims...       0   \n",
       "1164  How is #AndrewGillum plan on paying for the Mi...       1   \n",
       "1165  Really do tell! Hysterical? Strange tweet you ...       0   \n",
       "1166   In NY? Check out Immigrant Arts Coalition Summit       0   \n",
       "1167  I hate bitches that are always mad at the worl...       1   \n",
       "\n",
       "      Average_Word_Length  Lexical_Diversity  Capital_Letters_Count  \\\n",
       "0                0.076412           0.684766               0.039474   \n",
       "1                0.087043           0.962745               0.017544   \n",
       "2                0.050701           0.854220               0.013158   \n",
       "3                0.060908           1.000000               0.157895   \n",
       "4                0.086486           0.927894               0.065789   \n",
       "...                   ...                ...                    ...   \n",
       "1163             0.060997           0.955294               0.021930   \n",
       "1164             0.072390           0.882353               0.065789   \n",
       "1165             0.076887           1.000000               0.030702   \n",
       "1166             0.072259           1.000000               0.035088   \n",
       "1167             0.044850           1.000000               0.004386   \n",
       "\n",
       "      Hashtag_Frequency  Mention_Frequency  count_emojis  special_chars_count  \\\n",
       "0              0.000000                0.0           0.0             0.214286   \n",
       "1              0.000000                0.0           0.0             0.232143   \n",
       "2              0.000000                0.0           0.0             0.053571   \n",
       "3              0.000000                0.0           0.0             0.017857   \n",
       "4              0.074074                0.0           0.0             0.232143   \n",
       "...                 ...                ...           ...                  ...   \n",
       "1163           0.000000                0.0           0.0             0.053571   \n",
       "1164           0.222222                0.0           0.0             0.178571   \n",
       "1165           0.037037                0.0           0.0             0.071429   \n",
       "1166           0.000000                0.0           0.0             0.017857   \n",
       "1167           0.000000                0.0           0.0             0.000000   \n",
       "\n",
       "      Stop_Word_Frequency  Noun_Count  Verb_Count  Participle_Count  \\\n",
       "0                0.393939    0.476190    0.230769             0.125   \n",
       "1                0.393939    0.476190    0.153846             0.125   \n",
       "2                0.303030    0.238095    0.153846             0.000   \n",
       "3                0.060606    0.190476    0.153846             0.250   \n",
       "4                0.393939    0.285714    0.230769             0.125   \n",
       "...                   ...         ...         ...               ...   \n",
       "1163             0.363636    0.333333    0.230769             0.000   \n",
       "1164             0.606061    0.428571    0.461538             0.375   \n",
       "1165             0.090909    0.095238    0.230769             0.125   \n",
       "1166             0.060606    0.190476    0.000000             0.000   \n",
       "1167             0.272727    0.333333    0.076923             0.000   \n",
       "\n",
       "      Interjection_Count  Pronoun_Count  Preposition_Count  Adverb_Count  \\\n",
       "0                    0.0            0.0               0.00         0.000   \n",
       "1                    0.0            0.0               0.00         0.125   \n",
       "2                    0.0            0.0               0.25         0.000   \n",
       "3                    0.0            0.0               0.00         0.000   \n",
       "4                    0.0            0.0               0.00         0.000   \n",
       "...                  ...            ...                ...           ...   \n",
       "1163                 0.0            0.0               0.00         0.125   \n",
       "1164                 0.0            0.0               0.25         0.000   \n",
       "1165                 0.0            0.0               0.00         0.125   \n",
       "1166                 0.0            0.0               0.00         0.125   \n",
       "1167                 0.0            0.0               0.25         0.125   \n",
       "\n",
       "      Conjunction_Count  Readability_Score  URL_Frequency  \n",
       "0                   0.0           0.960962            0.0  \n",
       "1                   0.0           0.943398            0.0  \n",
       "2                   0.0           0.975479            0.0  \n",
       "3                   0.0           0.980919            0.0  \n",
       "4                   0.0           0.947917            0.0  \n",
       "...                 ...                ...            ...  \n",
       "1163                0.0           0.965321            0.0  \n",
       "1164                0.0           0.958714            0.0  \n",
       "1165                0.0           0.952009            0.0  \n",
       "1166                0.0           0.962357            0.0  \n",
       "1167                0.0           0.972415            0.0  \n",
       "\n",
       "[1168 rows x 20 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # do the same for the validation set\n",
    "# # features_list = [extract_features(tweet) for tweet in df_valid['text']]\n",
    "# # features_list = pd.DataFrame(features_list)\n",
    "# # df_valid = pd.concat([df_valid, features_list], axis=1)\n",
    "# # df_valid\n",
    "\n",
    "# feature_list = [extract_features(tweet) for tweet in df_valid['text']]\n",
    "# feature_list = pd.DataFrame(feature_list)\n",
    "# scaled_feature_list = scaler.transform(feature_list)\n",
    "# scaled_features_df = pd.DataFrame(scaled_feature_list, columns=feature_list.columns)\n",
    "# df_valid = pd.concat([df_valid, scaled_features_df], axis=1)\n",
    "# df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_valid.to_csv('valid.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>Average_Word_Length</th>\n",
       "      <th>Lexical_Diversity</th>\n",
       "      <th>Capital_Letters_Count</th>\n",
       "      <th>Hashtag_Frequency</th>\n",
       "      <th>Mention_Frequency</th>\n",
       "      <th>count_emojis</th>\n",
       "      <th>special_chars_count</th>\n",
       "      <th>Stop_Word_Frequency</th>\n",
       "      <th>Noun_Count</th>\n",
       "      <th>Verb_Count</th>\n",
       "      <th>Participle_Count</th>\n",
       "      <th>Interjection_Count</th>\n",
       "      <th>Pronoun_Count</th>\n",
       "      <th>Preposition_Count</th>\n",
       "      <th>Adverb_Count</th>\n",
       "      <th>Conjunction_Count</th>\n",
       "      <th>Readability_Score</th>\n",
       "      <th>URL_Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@user oh, you cute, you fancy bitch!</td>\n",
       "      <td>0</td>\n",
       "      <td>0.183455</td>\n",
       "      <td>0.814286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.876010</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I don't buy pussy cuz it's free bitch I buy lacs!</td>\n",
       "      <td>0</td>\n",
       "      <td>0.131855</td>\n",
       "      <td>0.763636</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.960009</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Such a trash episode of the walking dead tonight.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.202264</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.934676</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>big bitches be sayin why yo eyes all on my sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.137858</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.899334</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @user: Vice President Biden &amp;amp; his wife ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.199907</td>\n",
       "      <td>0.830435</td>\n",
       "      <td>0.158730</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778693</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6191</th>\n",
       "      <td>Bout tht time where the hoes are gettin dicked...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.177480</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.930696</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6192</th>\n",
       "      <td>RT @user: if we date u don't even gotta worry ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.133665</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.853355</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6193</th>\n",
       "      <td>RT @user: When your main bitch friends see you...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.153581</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.885347</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6194</th>\n",
       "      <td>Wake up American sheeple nd stop allowing thes...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.200226</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.828021</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6195</th>\n",
       "      <td>RT @user: Is it a crime if a nigga just wanna ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.120643</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.885347</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6196 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  labels  \\\n",
       "0                  @user oh, you cute, you fancy bitch!       0   \n",
       "1     I don't buy pussy cuz it's free bitch I buy lacs!       0   \n",
       "2     Such a trash episode of the walking dead tonight.       0   \n",
       "3     big bitches be sayin why yo eyes all on my sto...       0   \n",
       "4     RT @user: Vice President Biden &amp; his wife ...       0   \n",
       "...                                                 ...     ...   \n",
       "6191  Bout tht time where the hoes are gettin dicked...       0   \n",
       "6192  RT @user: if we date u don't even gotta worry ...       0   \n",
       "6193  RT @user: When your main bitch friends see you...       0   \n",
       "6194  Wake up American sheeple nd stop allowing thes...       0   \n",
       "6195  RT @user: Is it a crime if a nigga just wanna ...       0   \n",
       "\n",
       "      Average_Word_Length  Lexical_Diversity  Capital_Letters_Count  \\\n",
       "0                0.183455           0.814286               0.000000   \n",
       "1                0.131855           0.763636               0.031746   \n",
       "2                0.202264           1.000000               0.015873   \n",
       "3                0.137858           0.657895               0.015873   \n",
       "4                0.199907           0.830435               0.158730   \n",
       "...                   ...                ...                    ...   \n",
       "6191             0.177480           1.000000               0.015873   \n",
       "6192             0.133665           1.000000               0.047619   \n",
       "6193             0.153581           1.000000               0.047619   \n",
       "6194             0.200226           1.000000               0.031746   \n",
       "6195             0.120643           0.900000               0.047619   \n",
       "\n",
       "      Hashtag_Frequency  Mention_Frequency  count_emojis  special_chars_count  \\\n",
       "0                   0.0                0.1           0.0             0.088889   \n",
       "1                   0.0                0.0           0.0             0.066667   \n",
       "2                   0.0                0.0           0.0             0.022222   \n",
       "3                   0.0                0.0           0.0             0.022222   \n",
       "4                   0.0                0.1           0.0             0.088889   \n",
       "...                 ...                ...           ...                  ...   \n",
       "6191                0.0                0.0           0.0             0.000000   \n",
       "6192                0.0                0.1           0.0             0.088889   \n",
       "6193                0.0                0.1           0.0             0.155556   \n",
       "6194                0.0                0.0           0.0             0.044444   \n",
       "6195                0.0                0.1           0.0             0.066667   \n",
       "\n",
       "      Stop_Word_Frequency  Noun_Count  Verb_Count  Participle_Count  \\\n",
       "0                    0.10    0.090909       0.125               0.0   \n",
       "1                    0.20    0.181818       0.125               0.0   \n",
       "2                    0.20    0.136364       0.125               0.2   \n",
       "3                    0.45    0.272727       0.250               0.0   \n",
       "4                    0.40    0.545455       0.125               0.0   \n",
       "...                   ...         ...         ...               ...   \n",
       "6191                 0.20    0.090909       0.250               0.2   \n",
       "6192                 0.40    0.272727       0.500               0.2   \n",
       "6193                 0.25    0.272727       0.125               0.0   \n",
       "6194                 0.30    0.272727       0.250               0.2   \n",
       "6195                 0.30    0.227273       0.125               0.0   \n",
       "\n",
       "      Interjection_Count  Pronoun_Count  Preposition_Count  Adverb_Count  \\\n",
       "0                    0.0            0.0               0.00      0.000000   \n",
       "1                    0.0            0.0               0.00      0.000000   \n",
       "2                    0.0            0.0               0.00      0.000000   \n",
       "3                    0.0            0.0               0.00      0.000000   \n",
       "4                    0.0            0.0               0.00      0.000000   \n",
       "...                  ...            ...                ...           ...   \n",
       "6191                 0.0            0.0               0.25      0.000000   \n",
       "6192                 0.0            0.0               0.25      0.166667   \n",
       "6193                 0.0            0.0               0.00      0.000000   \n",
       "6194                 0.0            0.0               0.00      0.166667   \n",
       "6195                 0.0            0.0               0.00      0.166667   \n",
       "\n",
       "      Conjunction_Count  Readability_Score  URL_Frequency  \n",
       "0                   0.0           0.876010            0.0  \n",
       "1                   0.0           0.960009            0.0  \n",
       "2                   0.0           0.934676            0.0  \n",
       "3                   0.0           0.899334            0.0  \n",
       "4                   0.0           0.778693            0.0  \n",
       "...                 ...                ...            ...  \n",
       "6191                0.0           0.930696            0.0  \n",
       "6192                0.0           0.853355            0.0  \n",
       "6193                0.0           0.885347            0.0  \n",
       "6194                0.0           0.828021            0.0  \n",
       "6195                0.0           0.885347            0.0  \n",
       "\n",
       "[6196 rows x 20 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do the same for the test set\n",
    "# features_list = [extract_features(tweet) for tweet in df_test['text']]\n",
    "# features_list = pd.DataFrame(features_list)\n",
    "# df_test = pd.concat([df_test, features_list], axis=1)\n",
    "# df_test.head()\n",
    "\n",
    "feature_list = [extract_features(tweet) for tweet in df_test['text']]\n",
    "feature_list = pd.DataFrame(feature_list)\n",
    "scaled_feature_list = scaler.transform(feature_list)\n",
    "scaled_features_df = pd.DataFrame(scaled_feature_list, columns=feature_list.columns)\n",
    "df_test = pd.concat([df_test, scaled_features_df], axis=1)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
