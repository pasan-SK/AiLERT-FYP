{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 28.0/28.0 [00:00<00:00, 28.0kB/s]\n",
      "Downloading data: 100%|██████████| 1.13M/1.13M [00:01<00:00, 847kB/s]\n",
      "Downloading data: 100%|██████████| 145k/145k [00:00<00:00, 180kB/s]t]\n",
      "Downloading data: 100%|██████████| 340k/340k [00:00<00:00, 483kB/s]t]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.01it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 166.41it/s]\n",
      "Generating train split: 9088 examples [00:00, 53755.84 examples/s]\n",
      "Generating validation split: 1168 examples [00:00, 36184.23 examples/s]\n",
      "Generating test split: 2724 examples [00:00, 49060.61 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"krishan-CSE/HatEval-Relabeled\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "df_train = dataset['train'].to_pandas()\n",
    "df_valid = dataset['validation'].to_pandas()\n",
    "df_test = dataset['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9088, 2)\n",
      "train value_counts:\n",
      " labels\n",
      "0    4811\n",
      "1    4277\n",
      "Name: count, dtype: int64\n",
      "                                                text  labels\n",
      "0  This human-elephant conflict has seen 13 refug...       0\n",
      "1  The awkward moment when Lexus is showing you h...       0\n",
      "2            People- why are you so fucking mean Me-       0\n",
      "3  After EU uses Turkey as buffer to stop refugee...       0\n",
      "4                           Immigration in a picture       0\n",
      "====================================================================================\n",
      "(1168, 2)\n",
      "validation value_counts:\n",
      " labels\n",
      "0    618\n",
      "1    550\n",
      "Name: count, dtype: int64\n",
      "                                                text  labels\n",
      "0  President Jokowi: it's not true millions of Ch...       0\n",
      "1  So you created the problem by mass immigration...       1\n",
      "2  I though in a free country you could worship w...       0\n",
      "3  WELP. Bitch IM JUST NOW FUCKING SEEING DUMB WHORE       1\n",
      "4  .Considering THIS , the filth on the streets o...       1\n",
      "====================================================================================\n",
      "(2724, 2)\n",
      "test value_counts:\n",
      " labels\n",
      "0    1442\n",
      "1    1282\n",
      "Name: count, dtype: int64\n",
      "                                                text  labels\n",
      "0  We have got to get these Obama DACA illegal al...       1\n",
      "1  The same bitch is all on my boos shit like gir...       0\n",
      "2  BS WILSON IS A SKANK WHORE AND A LIAR . DIDDN'...       1\n",
      "3  Immigration Expert: Trudeau Has Lost Track Of ...       1\n",
      "4  I like to delete comments that say 'first' to ...       0\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(\"train value_counts:\\n\", df_train['labels'].value_counts())\n",
    "print(df_train.head())\n",
    "print(\"==========================================\"\n",
    "      \"==========================================\")\n",
    "print(df_valid.shape)\n",
    "print(\"validation value_counts:\\n\", df_valid['labels'].value_counts())\n",
    "print(df_valid.head())\n",
    "print(\"==========================================\"\n",
    "      \"==========================================\")\n",
    "print(df_test.shape)\n",
    "print(\"test value_counts:\\n\", df_test['labels'].value_counts())\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from textblob import TextBlob\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import emoji \n",
    "def average_word_length(tweet):\n",
    "    words = tweet.split()\n",
    "    return sum(len(word) for word in words) / len(words)\n",
    "\n",
    "# def average_sentence_length(tweet):\n",
    "#     sentences = re.split(r'[.!?]+', tweet)\n",
    "#     return sum(len(word_tokenize(sentence)) for sentence in sentences) / len(sentences)\n",
    "\n",
    "def lexical_diversity(tweet):\n",
    "    words = tweet.split()\n",
    "    unique_words = set(words)\n",
    "    return len(unique_words) / len(words)\n",
    "\n",
    "def count_capital_letters(tweet):\n",
    "    return sum(1 for char in tweet if char.isupper())\n",
    "\n",
    "def count_words_surrounded_by_colons(tweet):\n",
    "    # Define a regular expression pattern to match words surrounded by ':'\n",
    "    pattern = r':(\\w+):'\n",
    "\n",
    "    # Use re.findall to find all matches in the tweet\n",
    "    matches = re.findall(pattern, tweet)\n",
    "\n",
    "    # Return the count of matched words\n",
    "    return len(matches)\n",
    "\n",
    "def count_emojis(tweet):\n",
    "    # Convert emoji symbols to their corresponding names\n",
    "    tweet_with_names = emoji.demojize(tweet)\n",
    "    return count_words_surrounded_by_colons(tweet_with_names)\n",
    "\n",
    "def hashtag_frequency(tweet):\n",
    "    hashtags = re.findall(r'#\\w+', tweet)\n",
    "    return len(hashtags)\n",
    "\n",
    "def mention_frequency(tweet):\n",
    "    mentions = re.findall(r'@\\w+', tweet)\n",
    "    return len(mentions)\n",
    "\n",
    "import string\n",
    "\n",
    "def count_special_characters(tweet):\n",
    "    special_characters = [char for char in tweet if char in string.punctuation]\n",
    "    return len(special_characters)\n",
    "\n",
    "# def capitalization_pattern(tweet):\n",
    "#     if tweet.islower():\n",
    "#         return 'All Lowercase'\n",
    "#     elif tweet.isupper():\n",
    "#         return 'All Uppercase'\n",
    "#     elif tweet.istitle():\n",
    "#         return 'Title Case'\n",
    "#     else:\n",
    "#         return 'Mixed Case'\n",
    "\n",
    "def stop_word_frequency(tweet):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in tweet.split() if word.lower() in stop_words]\n",
    "    return len(words)\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def get_linguistic_features(tweet):\n",
    "    # Tokenize the tweet\n",
    "    words = word_tokenize(tweet)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "\n",
    "    # Get parts of speech tags\n",
    "    pos_tags = pos_tag(filtered_words)\n",
    "\n",
    "    # Count various linguistic features\n",
    "    noun_count = sum(1 for word, pos in pos_tags if pos.startswith('N'))\n",
    "    verb_count = sum(1 for word, pos in pos_tags if pos.startswith('V'))\n",
    "    participle_count = sum(1 for word, pos in pos_tags if pos.startswith('V') and ('ing' in word or 'ed' in word))\n",
    "    interjection_count = sum(1 for word, pos in pos_tags if pos == 'UH')\n",
    "    pronoun_count = sum(1 for word, pos in pos_tags if pos.startswith('PRP'))\n",
    "    preposition_count = sum(1 for word, pos in pos_tags if pos.startswith('IN'))\n",
    "    adverb_count = sum(1 for word, pos in pos_tags if pos.startswith('RB'))\n",
    "    conjunction_count = sum(1 for word, pos in pos_tags if pos.startswith('CC'))\n",
    "\n",
    "    return {\n",
    "        'Noun_Count': noun_count,\n",
    "        'Verb_Count': verb_count,\n",
    "        'Participle_Count': participle_count,\n",
    "        'Interjection_Count': interjection_count,\n",
    "        'Pronoun_Count': pronoun_count,\n",
    "        'Preposition_Count': preposition_count,\n",
    "        'Adverb_Count': adverb_count,\n",
    "        'Conjunction_Count': conjunction_count\n",
    "    }\n",
    "\n",
    "import textstat\n",
    "def readability_score(tweet):\n",
    "    return textstat.flesch_reading_ease(tweet)\n",
    "\n",
    "def get_url_frequency(tweet):\n",
    "    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', tweet)\n",
    "    return len(urls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn textblob nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df_train.drop('labels', axis=1)\n",
    "y = df_train['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "9083    1\n",
       "9084    1\n",
       "9085    0\n",
       "9086    1\n",
       "9087    1\n",
       "Name: labels, Length: 9088, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define a function to extract features from a single tweet\n",
    "def extract_features(tweet):\n",
    "    features = {\n",
    "        'Average_Word_Length': average_word_length(tweet),\n",
    "        # 'Average_Sentence_Length': average_sentence_length(tweet),\n",
    "        'Lexical_Diversity': lexical_diversity(tweet),\n",
    "        'Capital_Letters_Count': count_capital_letters(tweet),  # Uncomment if you want to include this feature\n",
    "        'Hashtag_Frequency': hashtag_frequency(tweet),\n",
    "        'Mention_Frequency': mention_frequency(tweet),\n",
    "        'count_emojis': count_emojis(tweet),\n",
    "        'special_chars_count': count_special_characters(tweet),\n",
    "        'Stop_Word_Frequency': stop_word_frequency(tweet),\n",
    "        **get_linguistic_features(tweet),  # Include linguistic features\n",
    "        'Readability_Score': readability_score(tweet),\n",
    "        'URL_Frequency': get_url_frequency(tweet)  # Assuming you have the correct function for this\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# Extract features for all tweets\n",
    "features_list = [extract_features(tweet) for tweet in X['text']]\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "X_new = pd.DataFrame(features_list)\n",
    "\n",
    "# # Add the labels to the DataFrame\n",
    "# df['Label'] = labels\n",
    "\n",
    "# # Display the DataFrame\n",
    "# print(df)\n",
    "\n",
    "# The progress from beginning\n",
    "# The initial phase is to do a comprehensive literature review on the domain of hate speech detection. Since our group consists of 3 people, we could collectively gothrough more than 50 papers and as a group we discussed and share knowledge between ourselves to collectively gain and improve our knowledge in this domain. While going through the papers, we collected the openly avaialable datasets to help in our initial experiments. We went through different models, features, intepreatbility techniques, evaluationmetrics, and pre-processing techniques. We also went through the different types of hate speech and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Average_Word_Length</th>\n",
       "      <th>Lexical_Diversity</th>\n",
       "      <th>Capital_Letters_Count</th>\n",
       "      <th>Hashtag_Frequency</th>\n",
       "      <th>Mention_Frequency</th>\n",
       "      <th>count_emojis</th>\n",
       "      <th>special_chars_count</th>\n",
       "      <th>Stop_Word_Frequency</th>\n",
       "      <th>Noun_Count</th>\n",
       "      <th>Verb_Count</th>\n",
       "      <th>Participle_Count</th>\n",
       "      <th>Interjection_Count</th>\n",
       "      <th>Pronoun_Count</th>\n",
       "      <th>Preposition_Count</th>\n",
       "      <th>Adverb_Count</th>\n",
       "      <th>Conjunction_Count</th>\n",
       "      <th>Readability_Score</th>\n",
       "      <th>URL_Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.894737</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>78.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.437500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>88.74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.777778</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42.72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9083</th>\n",
       "      <td>4.619048</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>67.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9084</th>\n",
       "      <td>8.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9085</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>54.22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9086</th>\n",
       "      <td>3.428571</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>100.92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9087</th>\n",
       "      <td>7.631579</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9088 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Average_Word_Length  Lexical_Diversity  Capital_Letters_Count  \\\n",
       "0                6.894737           1.000000                      3   \n",
       "1                4.437500           1.000000                      2   \n",
       "2                4.000000           1.000000                      2   \n",
       "3                4.777778           0.844444                     10   \n",
       "4                5.250000           1.000000                      1   \n",
       "...                   ...                ...                    ...   \n",
       "9083             4.619048           0.857143                      4   \n",
       "9084             8.700000           1.000000                      9   \n",
       "9085             6.000000           0.970588                     10   \n",
       "9086             3.428571           0.952381                      3   \n",
       "9087             7.631579           0.894737                     40   \n",
       "\n",
       "      Hashtag_Frequency  Mention_Frequency  count_emojis  special_chars_count  \\\n",
       "0                     4                  0             0                    6   \n",
       "1                     0                  0             0                    3   \n",
       "2                     0                  0             0                    2   \n",
       "3                     0                  0             0                    6   \n",
       "4                     0                  0             0                    0   \n",
       "...                 ...                ...           ...                  ...   \n",
       "9083                  1                  0             0                    1   \n",
       "9084                  3                  0             0                    4   \n",
       "9085                  0                  0             0                   14   \n",
       "9086                  0                  0             0                    2   \n",
       "9087                  6                  0             0                    9   \n",
       "\n",
       "      Stop_Word_Frequency  Noun_Count  Verb_Count  Participle_Count  \\\n",
       "0                       3           6           2                 1   \n",
       "1                       7           4           3                 1   \n",
       "2                       4           1           1                 1   \n",
       "3                      15          14          11                 4   \n",
       "4                       2           2           0                 0   \n",
       "...                   ...         ...         ...               ...   \n",
       "9083                    9           6           2                 1   \n",
       "9084                    4           4           2                 0   \n",
       "9085                   10           8           5                 2   \n",
       "9086                   11           4           1                 0   \n",
       "9087                    4           7           5                 0   \n",
       "\n",
       "      Interjection_Count  Pronoun_Count  Preposition_Count  Adverb_Count  \\\n",
       "0                      0              0                  1             1   \n",
       "1                      0              0                  0             0   \n",
       "2                      0              0                  0             0   \n",
       "3                      0              0                  0             0   \n",
       "4                      0              0                  0             0   \n",
       "...                  ...            ...                ...           ...   \n",
       "9083                   0              0                  0             2   \n",
       "9084                   0              0                  0             0   \n",
       "9085                   0              0                  0             3   \n",
       "9086                   0              0                  0             1   \n",
       "9087                   0              0                  0             0   \n",
       "\n",
       "      Conjunction_Count  Readability_Score  URL_Frequency  \n",
       "0                     0              78.75              0  \n",
       "1                     0              80.62              0  \n",
       "2                     0              88.74              0  \n",
       "3                     0              42.72              0  \n",
       "4                     0              33.58              0  \n",
       "...                 ...                ...            ...  \n",
       "9083                  0              67.08              0  \n",
       "9084                  0              10.56              0  \n",
       "9085                  0              54.22              0  \n",
       "9086                  0             100.92              0  \n",
       "9087                  0              36.45              0  \n",
       "\n",
       "[9088 rows x 18 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.62\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.74      0.67       948\n",
      "           1       0.64      0.49      0.55       870\n",
      "\n",
      "    accuracy                           0.62      1818\n",
      "   macro avg       0.63      0.62      0.61      1818\n",
      "weighted avg       0.62      0.62      0.62      1818\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We have got to get these Obama DACA illegal al...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The same bitch is all on my boos shit like gir...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BS WILSON IS A SKANK WHORE AND A LIAR . DIDDN'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Immigration Expert: Trudeau Has Lost Track Of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I like to delete comments that say 'first' to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  labels\n",
       "0  We have got to get these Obama DACA illegal al...       1\n",
       "1  The same bitch is all on my boos shit like gir...       0\n",
       "2  BS WILSON IS A SKANK WHORE AND A LIAR . DIDDN'...       1\n",
       "3  Immigration Expert: Trudeau Has Lost Track Of ...       1\n",
       "4  I like to delete comments that say 'first' to ...       0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.66\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.76      0.70      1442\n",
      "           1       0.67      0.54      0.60      1282\n",
      "\n",
      "    accuracy                           0.66      2724\n",
      "   macro avg       0.66      0.65      0.65      2724\n",
      "weighted avg       0.66      0.66      0.65      2724\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_list_test = [extract_features(tweet) for tweet in df_test['text']]\n",
    "X_test_new = pd.DataFrame(features_list_test)\n",
    "X_test_scaled = scaler.transform(X_test_new)\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "accuracy_test = accuracy_score(df_test['labels'], y_test_pred)\n",
    "print(f\"Test Accuracy: {accuracy_test:.2f}\")\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(df_test['labels'], y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
