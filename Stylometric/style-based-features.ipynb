{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Owner\\anaconda3\\envs\\emotion\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"krishan-CSE/HatEval_New\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "df_train = dataset['train'].to_pandas()\n",
    "df_valid = dataset['validation'].to_pandas()\n",
    "df_test = dataset['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 2)\n",
      "train value_counts:\n",
      " labels\n",
      "0    5201\n",
      "1    3781\n",
      "Name: count, dtype: int64\n",
      "                                                text  labels\n",
      "0  Hurray, saving us $$$ in so many ways #LockThe...       1\n",
      "1  Why would young fighting age men be the vast m...       1\n",
      "2  Illegals Dump their Kids at the border like Ro...       1\n",
      "3  NY Times: 'Nearly All White' States Pose 'an A...       0\n",
      "4  Orban in Brussels: European leaders are ignori...       0\n",
      "====================================================================================\n",
      "(998, 2)\n",
      "validation value_counts:\n",
      " labels\n",
      "0    571\n",
      "1    427\n",
      "Name: count, dtype: int64\n",
      "                                                text  labels\n",
      "0  I swear Im getting to places just in the nick ...       0\n",
      "1  Im an immigrant and Trump is right on immigrat...       0\n",
      "2  #IllegalImmigrants #IllegalAliens #ElectoralSy...       1\n",
      "3  We have our own invasion issues with Mexicans....       1\n",
      "4  Worker Charged With Sexually Molesting Eight C...       0\n",
      "====================================================================================\n",
      "(3000, 2)\n",
      "test value_counts:\n",
      " labels\n",
      "1    1901\n",
      "0    1099\n",
      "Name: count, dtype: int64\n",
      "                                                text  labels\n",
      "0  Oh, I could have gone on about taxes. Since th...       0\n",
      "1  Several of the wild fires in #california and #...       1\n",
      "2  My question is how do you resettle a refugee a...       0\n",
      "3  #Europe, you've got a problem! We must hurry a...       1\n",
      "4  This is outrageous! #StopIllegalImmigration #M...       1\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(\"train value_counts:\\n\", df_train['labels'].value_counts())\n",
    "print(df_train.head())\n",
    "print(\"==========================================\"\n",
    "      \"==========================================\")\n",
    "print(df_valid.shape)\n",
    "print(\"validation value_counts:\\n\", df_valid['labels'].value_counts())\n",
    "print(df_valid.head())\n",
    "print(\"==========================================\"\n",
    "      \"==========================================\")\n",
    "print(df_test.shape)\n",
    "print(\"test value_counts:\\n\", df_test['labels'].value_counts())\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from textblob import TextBlob\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import emoji \n",
    "def average_word_length(tweet):\n",
    "    words = tweet.split()\n",
    "    return sum(len(word) for word in words) / len(words)\n",
    "\n",
    "# def average_sentence_length(tweet):\n",
    "#     sentences = re.split(r'[.!?]+', tweet)\n",
    "#     return sum(len(word_tokenize(sentence)) for sentence in sentences) / len(sentences)\n",
    "\n",
    "def lexical_diversity(tweet):\n",
    "    words = tweet.split()\n",
    "    unique_words = set(words)\n",
    "    return len(unique_words) / len(words)\n",
    "\n",
    "def count_capital_letters(tweet):\n",
    "    return sum(1 for char in tweet if char.isupper())\n",
    "\n",
    "def count_words_surrounded_by_colons(tweet):\n",
    "    # Define a regular expression pattern to match words surrounded by ':'\n",
    "    pattern = r':(\\w+):'\n",
    "\n",
    "    # Use re.findall to find all matches in the tweet\n",
    "    matches = re.findall(pattern, tweet)\n",
    "\n",
    "    # Return the count of matched words\n",
    "    return len(matches)\n",
    "\n",
    "def count_emojis(tweet):\n",
    "    # Convert emoji symbols to their corresponding names\n",
    "    tweet_with_names = emoji.demojize(tweet)\n",
    "    return count_words_surrounded_by_colons(tweet_with_names)\n",
    "\n",
    "def hashtag_frequency(tweet):\n",
    "    hashtags = re.findall(r'#\\w+', tweet)\n",
    "    return len(hashtags)\n",
    "\n",
    "def mention_frequency(tweet):\n",
    "    mentions = re.findall(r'@\\w+', tweet)\n",
    "    return len(mentions)\n",
    "\n",
    "import string\n",
    "\n",
    "def count_special_characters(tweet):\n",
    "    special_characters = [char for char in tweet if char in string.punctuation]\n",
    "    return len(special_characters)\n",
    "\n",
    "# def capitalization_pattern(tweet):\n",
    "#     if tweet.islower():\n",
    "#         return 'All Lowercase'\n",
    "#     elif tweet.isupper():\n",
    "#         return 'All Uppercase'\n",
    "#     elif tweet.istitle():\n",
    "#         return 'Title Case'\n",
    "#     else:\n",
    "#         return 'Mixed Case'\n",
    "\n",
    "def stop_word_frequency(tweet):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in tweet.split() if word.lower() in stop_words]\n",
    "    return len(words)\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def get_linguistic_features(tweet):\n",
    "    # Tokenize the tweet\n",
    "    words = word_tokenize(tweet)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "\n",
    "    # Get parts of speech tags\n",
    "    pos_tags = pos_tag(filtered_words)\n",
    "\n",
    "    # Count various linguistic features\n",
    "    noun_count = sum(1 for word, pos in pos_tags if pos.startswith('N'))\n",
    "    verb_count = sum(1 for word, pos in pos_tags if pos.startswith('V'))\n",
    "    participle_count = sum(1 for word, pos in pos_tags if pos.startswith('V') and ('ing' in word or 'ed' in word))\n",
    "    interjection_count = sum(1 for word, pos in pos_tags if pos == 'UH')\n",
    "    pronoun_count = sum(1 for word, pos in pos_tags if pos.startswith('PRP'))\n",
    "    preposition_count = sum(1 for word, pos in pos_tags if pos.startswith('IN'))\n",
    "    adverb_count = sum(1 for word, pos in pos_tags if pos.startswith('RB'))\n",
    "    conjunction_count = sum(1 for word, pos in pos_tags if pos.startswith('CC'))\n",
    "\n",
    "    return {\n",
    "        'Noun_Count': noun_count,\n",
    "        'Verb_Count': verb_count,\n",
    "        'Participle_Count': participle_count,\n",
    "        'Interjection_Count': interjection_count,\n",
    "        'Pronoun_Count': pronoun_count,\n",
    "        'Preposition_Count': preposition_count,\n",
    "        'Adverb_Count': adverb_count,\n",
    "        'Conjunction_Count': conjunction_count\n",
    "    }\n",
    "\n",
    "import textstat\n",
    "def readability_score(tweet):\n",
    "    return textstat.flesch_reading_ease(tweet)\n",
    "\n",
    "def get_url_frequency(tweet):\n",
    "    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', tweet)\n",
    "    return len(urls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn textblob nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df_train.drop('labels', axis=1)\n",
    "y = df_train['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "8977    0\n",
       "8978    0\n",
       "8979    1\n",
       "8980    0\n",
       "8981    0\n",
       "Name: labels, Length: 8982, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define a function to extract features from a single tweet\n",
    "def extract_features(tweet):\n",
    "    features = {\n",
    "        'Average_Word_Length': average_word_length(tweet),\n",
    "        # 'Average_Sentence_Length': average_sentence_length(tweet),\n",
    "        'Lexical_Diversity': lexical_diversity(tweet),\n",
    "        'Capital_Letters_Count': count_capital_letters(tweet),  # Uncomment if you want to include this feature\n",
    "        'Hashtag_Frequency': hashtag_frequency(tweet),\n",
    "        'Mention_Frequency': mention_frequency(tweet),\n",
    "        'count_emojis': count_emojis(tweet),\n",
    "        'special_chars_count': count_special_characters(tweet),\n",
    "        'Stop_Word_Frequency': stop_word_frequency(tweet),\n",
    "        **get_linguistic_features(tweet),  # Include linguistic features\n",
    "        'Readability_Score': readability_score(tweet),\n",
    "        'URL_Frequency': get_url_frequency(tweet)  # Assuming you have the correct function for this\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# Extract features for all tweets\n",
    "features_list = [extract_features(tweet) for tweet in X['text']]\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "X_new = pd.DataFrame(features_list)\n",
    "\n",
    "# # Add the labels to the DataFrame\n",
    "# df['Label'] = labels\n",
    "\n",
    "# # Display the DataFrame\n",
    "# print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Average_Word_Length</th>\n",
       "      <th>Lexical_Diversity</th>\n",
       "      <th>Capital_Letters_Count</th>\n",
       "      <th>Hashtag_Frequency</th>\n",
       "      <th>Mention_Frequency</th>\n",
       "      <th>count_emojis</th>\n",
       "      <th>special_chars_count</th>\n",
       "      <th>Stop_Word_Frequency</th>\n",
       "      <th>Noun_Count</th>\n",
       "      <th>Verb_Count</th>\n",
       "      <th>Participle_Count</th>\n",
       "      <th>Interjection_Count</th>\n",
       "      <th>Pronoun_Count</th>\n",
       "      <th>Preposition_Count</th>\n",
       "      <th>Adverb_Count</th>\n",
       "      <th>Conjunction_Count</th>\n",
       "      <th>Readability_Score</th>\n",
       "      <th>URL_Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.538462</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42.38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.893617</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>56.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.205128</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>60.14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.076923</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>75.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.823529</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8977</th>\n",
       "      <td>3.625000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71.82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8978</th>\n",
       "      <td>4.466667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64.71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8979</th>\n",
       "      <td>4.322581</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>69.48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8980</th>\n",
       "      <td>6.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8981</th>\n",
       "      <td>4.916667</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>65.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8982 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Average_Word_Length  Lexical_Diversity  Capital_Letters_Count  \\\n",
       "0                6.538462           1.000000                     18   \n",
       "1                4.893617           0.808511                      3   \n",
       "2                5.205128           0.923077                     34   \n",
       "3                5.076923           1.000000                     11   \n",
       "4                4.823529           0.941176                      3   \n",
       "...                   ...                ...                    ...   \n",
       "8977             3.625000           1.000000                      1   \n",
       "8978             4.466667           1.000000                      1   \n",
       "8979             4.322581           0.903226                      6   \n",
       "8980             6.333333           1.000000                      1   \n",
       "8981             4.916667           0.958333                      3   \n",
       "\n",
       "      Hashtag_Frequency  Mention_Frequency  count_emojis  special_chars_count  \\\n",
       "0                     5                  0             0                    9   \n",
       "1                     0                  0             0                   10   \n",
       "2                     4                  0             0                    6   \n",
       "3                     0                  0             0                    5   \n",
       "4                     0                  0             0                    2   \n",
       "...                 ...                ...           ...                  ...   \n",
       "8977                  0                  0             0                    1   \n",
       "8978                  0                  0             0                    1   \n",
       "8979                  0                  0             0                    3   \n",
       "8980                  0                  0             0                    0   \n",
       "8981                  1                  0             0                   10   \n",
       "\n",
       "      Stop_Word_Frequency  Noun_Count  Verb_Count  Participle_Count  \\\n",
       "0                       2           5           2                 1   \n",
       "1                      21          11           6                 3   \n",
       "2                      15          12           4                 0   \n",
       "3                       3           4           1                 0   \n",
       "4                       9           4           2                 1   \n",
       "...                   ...         ...         ...               ...   \n",
       "8977                    5           1           0                 0   \n",
       "8978                    7           6           0                 0   \n",
       "8979                   11           7           7                 5   \n",
       "8980                    0           1           0                 0   \n",
       "8981                    8           4           2                 1   \n",
       "\n",
       "      Interjection_Count  Pronoun_Count  Preposition_Count  Adverb_Count  \\\n",
       "0                      0              1                  0             0   \n",
       "1                      0              0                  1             2   \n",
       "2                      0              0                  1             1   \n",
       "3                      0              0                  0             0   \n",
       "4                      0              0                  0             0   \n",
       "...                  ...            ...                ...           ...   \n",
       "8977                   0              0                  0             0   \n",
       "8978                   0              0                  0             0   \n",
       "8979                   0              0                  0             2   \n",
       "8980                   0              0                  1             0   \n",
       "8981                   1              0                  0             3   \n",
       "\n",
       "      Conjunction_Count  Readability_Score  URL_Frequency  \n",
       "0                     0              42.38              0  \n",
       "1                     0              56.08              0  \n",
       "2                     0              60.14              0  \n",
       "3                     0              75.20              0  \n",
       "4                     0              62.68              0  \n",
       "...                 ...                ...            ...  \n",
       "8977                  0              71.82              0  \n",
       "8978                  0              64.71              0  \n",
       "8979                  0              69.48              0  \n",
       "8980                  0               9.21              0  \n",
       "8981                  0              65.05              0  \n",
       "\n",
       "[8982 rows x 18 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.66\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.81      0.73      1042\n",
      "           1       0.63      0.44      0.52       755\n",
      "\n",
      "    accuracy                           0.66      1797\n",
      "   macro avg       0.65      0.63      0.63      1797\n",
      "weighted avg       0.65      0.66      0.64      1797\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oh, I could have gone on about taxes. Since th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Several of the wild fires in #california and #...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My question is how do you resettle a refugee a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Europe, you've got a problem! We must hurry a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is outrageous! #StopIllegalImmigration #M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  labels\n",
       "0  Oh, I could have gone on about taxes. Since th...       0\n",
       "1  Several of the wild fires in #california and #...       1\n",
       "2  My question is how do you resettle a refugee a...       0\n",
       "3  #Europe, you've got a problem! We must hurry a...       1\n",
       "4  This is outrageous! #StopIllegalImmigration #M...       1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.64\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.58      0.54      1099\n",
      "           1       0.73      0.67      0.70      1901\n",
      "\n",
      "    accuracy                           0.64      3000\n",
      "   macro avg       0.62      0.62      0.62      3000\n",
      "weighted avg       0.65      0.64      0.64      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_list_test = [extract_features(tweet) for tweet in df_test['text']]\n",
    "X_test_new = pd.DataFrame(features_list_test)\n",
    "X_test_scaled = scaler.transform(X_test_new)\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "accuracy_test = accuracy_score(df_test['labels'], y_test_pred)\n",
    "print(f\"Test Accuracy: {accuracy_test:.2f}\")\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(df_test['labels'], y_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
